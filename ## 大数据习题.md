## 大数据习题



### 1. Spark rdd生成过程

Spark的任务调度分为四步

1RDD objects

RDD的准备阶段，组织RDD及RDD的依赖关系生成大概的RDD的DAG图，DAG图是有向环图。

2DAG scheduler

细分RDD中partition的依赖关系确定那些是宽依赖那些是窄依赖，生成更详细的DAG图，将DAG图封装成 TaskSet任务集合当触发计算时(执行action型算子)将其提交给集群。

3TaskScheduler

接收TaskSet任务集，分析确定那个task对应那个worker并将其发送给worker执行。

4worker执行阶段

接收task任务，通过spark的block管理器blockManager从集群节点上获取对应的block上的数据，启动executor完成计算

### 2. Spark 任务提交流程





2.spark-submit命令提交程序后，driver和application也会向Master注册信息

3.创建SparkContext对象：主要的对象包含DAGScheduler和TaskScheduler

4.Driver把Application信息注册给Master后，Master会根据App信息去Worker节点启动Executor

5.Executor内部会创建运行task的线程池，然后把启动的Executor反向注册给Dirver

6.DAGScheduler：负责把Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；

同时DAGScheduler还会处理由于Shuffle数据丢失导致的失败；

7.TaskScheduler：维护所有TaskSet，分发Task给各个节点的Executor（根据数据本地化策略分发Task），监控task的运行状态，负责重试失败的task；

8.所有task运行完成后，SparkContext向Master注销，释放资源；


### 3.Spark sql 创建分区表

spark.sql("use oracledb")

spark.sql("CREATE TABLE IF NOT EXISTS " + tablename + " (OBUID STRING, BUS_ID STRING,REVTIME STRING,OBUTIME STRING,LONGITUDE STRING,LATITUDE STRING,\

GPSKEY STRING,DIRECTION STRING,SPEED STRING,RUNNING_NO STRING,DATA_SERIAL STRING,GPS_MILEAGE STRING,SATELLITE_COUNT STRING,ROUTE_CODE STRING,SERVICE STRING)\

PARTITIONED BY(area STRING,obudate STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ")

\# 设置参数

\# hive > set hive.exec.dynamic.partition.mode = nonstrict;

\# hive > set hive.exec.dynamic.partition = true;

spark.sql("set hive.exec.dynamic.partition.mode = nonstrict")

spark.sql("set hive.exec.dynamic.partition = true")

\# print("创建数据库完成")

if addoroverwrite:

\# 追加

spark.sql("INSERT INTO TABLE " + tablename + " PARTITION(area,obudate) SELECT OBUID,BUS_ID, REVTIME, OBUTIME,LONGITUDE ,LATITUDE,GPSKEY,DIRECTION,SPEED,\

RUNNING_NO,DATA_SERIAL,GPS_MILEAGE, SATELLITE_COUNT ,ROUTE_CODE,SERVICE,'gz' AS area ,SUBSTR(OBUTIME,1,10) AS obudate FROM " + tablename + "_tmp")

### 4.Java 同步锁有哪些

Synchronized lock

### 5.Arrarylist 能存null吗

可以 添加的数据类型位object类型 null也是object类型

### 6. Spring cloud 控制权限

Spring Cloud下的微服务权限怎么管？怎么设计比较合理？从大层面讲叫服务权限，往小处拆分，分别为三块：用户认证、用户权限、服务校验。

### 7. Hashset contains方法

contains方法用来判断Set集合是否包含指定的对象。

语法 boolean contains(Object o)

返回值：如果Set集合包含指定的对象，则返回true；否则返回false。





### 8. Spark streaming 数据块大小

buffer 32k //缓冲区默认大小为32k SparkConf.set("spark.shuffle.file.buffer","64k")

reduce 48M //reduce端拉取数据的时候，默认大小是48M SparkConf.set("spark.reducer.maxSizeInFlight","96M")

spark.shuffle.file.buffer

默认值：32k

参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。

调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。

spark.reducer.maxSizeInFlight

默认值：48m

参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。

调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。

错误：reduce oom

reduce task去map拉数据，reduce 一边拉数据一边聚合 reduce段有一块聚合内存（executor memory * 0.2）

解决办法：1、增加reduce 聚合的内存的比例 设置spark.shuffle.memoryFraction

2、 增加executor memory的大小 --executor-memory 5G

3、减少reduce task每次拉取的数据量 设置spark.reducer.maxSizeInFlight 24m

### 9.GC

Java GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，在使用JAVA的时候，一般不需要专门编写内存回收和垃圾清理代 码。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。

### 10.Flume 的推拉 怎么保证数据的完整性

channel做持久化

### 11. Java 1/0.0 infinity

在浮点数运算时，有时我们会遇到除数为0的情况，那java是如何解决的呢？

我们知道，在整型运算中，除数是不能为0的，否则直接运行异常。但是在浮点数运算中，引入了无限这个概念，我们来看一下Double和Float中的定义。

\1. 无限乘以0，结果为NAN

System.out.println(Float.POSITIVE_INFINITY * 0); // output: NAN

System.out.println(Float.NEGATIVE_INFINITY * 0); // output: NAN

2.无限除以0，结果不变，还是无限

System.out.println((Float.POSITIVE_INFINITY / 0) == Float.POSITIVE_INFINITY); // output: true

System.out.println((Float.NEGATIVE_INFINITY / 0) == Float.NEGATIVE_INFINITY); // output: true

3.无限做除了乘以0意外的运算，结果还是无限

System.out.println(Float.POSITIVE_INFINITY == (Float.POSITIVE_INFINITY + 10000)); // output: true

System.out.println(Float.POSITIVE_INFINITY == (Float.POSITIVE_INFINITY - 10000)); // output: true

System.out.println(Float.POSITIVE_INFINITY == (Float.POSITIVE_INFINITY * 10000)); // output: true

System.out.println(Float.POSITIVE_INFINITY == (Float.POSITIVE_INFINITY / 10000)); // output: true

要判断一个浮点数是否为INFINITY，可用isInfinite方法

System.out.println(Double.isInfinite(Float.POSITIVE_INFINITY)); // output: true

### 12. (int) (char) (byte) -1 = 65535

public class T {

public static void main(String args[]) {

new T().toInt(-1);

new T().toByte((byte) -1);

new T().toChar((char) (byte) -1);

new T().toInt((int)(char) (byte) -1);

}

void toByte(byte b) {

for (int i = 7; i >= 0; i--) {

System.out.print((b>>i) & 0x01);

}

System.out.println();

}

void toInt(int b) {

for (int i = 31; i >= 0; i--) {

System.out.print((b>>i) & 0x01);

}

System.out.println();

}

void toChar(char b) {

for (int i = 15; i >= 0; i--) {

System.out.print((b>>i) & 0x01);

}

System.out.println();

}

}

11111111111111111111111111111111

11111111

1111111111111111

00000000000000001111111111111111

### 13. Spark shuffle时 是否会在磁盘存储
 会

### 14. Hive的函数

例如case when

### 15. Hadoop 的shuffle 会进行几次排序

### 16. Shuffle 发生在哪里

hadoop的核心思想是MapReduce，但shuffle又是MapReduce的核心。shuffle的主要工作是从Map结束到Reduce开始之间的过程。首先看下这张图，就能了解shuffle所处的位置。图中的partitions、copy phase、sort phase所代表的就是shuffle的不同阶段。

### 17. spark怎么杀死已经提交的任务

### 18. 提交spark 任务可以设置哪些参数

### 19. Zookeeper 有3个进程都是做什么的

Zookeeper主要可以干哪些事情：配置管理，名字服务，提供分布式同步以及集群管理。

### 20. Kafka 的三种传数据的方式，各有什么优缺点

1.最多一次（At-most-once）：客户端收到消息后，在处理消息前自动提交，这样kafka就认为consumer已经消费过了，偏移量增加。

2.最少一次(At-least-once)：客户端收到消息，处理消息，再提交反馈。这样就可能出现消息处理完了，在提交反馈前，网络中断或者程序挂了，那么kafka认为这个消息还没有被consumer消费，产生重复消息推送。

3.正好一次(Exaxtly-once)：保证消息处理和提交反馈在同一个事务中，即有原子性。

本文从这几个点出发，详细阐述了如何实现以上三种方式。

### 21. Flume 的拦截插件怎么编写

建一个maven工程，导入flume-core包，然后实现interceptor接口

### 22. Hadoop 的小文件聚合怎么实现

Hadoop 自身提供了几种机制来解决相关的问题，包括HAR， SequeueFile和CombineFileInputFormat。

### 23. Spark rdd 存储数据吗？

RDD其实是不存储真实数据的，存储的的只是真实数据的分区信息getPartitions，还有就是针对单个分区的读取方法 compute

### 24. 实现map的线程同步方法

实现同步机制有两个方法：

1、同步代码块：

synchronized(同一个数据){} 同一个数据：就是N条线程同时访问一个数据。

2、同步方法：

public synchronized 数据返回类型 方法名(){}

就是使用 synchronized 来修饰某个方法，则该方法称为同步方法。对于同步方法而言，无需显示指定同步监视器，同步方法的同步监视器是 this 也就是该对象的本身（这里指的对象本身有点含糊，其实就是调用该同步方法的对象）通过使用同步方法，可非常方便的将某类变成线程安全的类，具有如下特征：

1，该类的对象可以被多个线程安全的访问。

2，每个线程调用该对象的任意方法之后，都将得到正确的结果。

3，每个线程调用该对象的任意方法之后，该对象状态依然保持合理状态。

注：synchronized关键字可以修饰方法，也可以修饰代码块，但不能修饰构造器，属性等。

实现同步机制注意以下几点： 安全性高，性能低，在多线程用。性能高，安全性低，在单线程用。

1，不要对线程安全类的所有方法都进行同步，只对那些会改变共享资源方法的进行同步。

2，如果可变类有两种运行环境，当线程环境和多线程环境则应该为该可变类提供两种版本：线程安全版本和线程不安全版本(没有同步方法和同步块)。在单线程中环境中，使用线程不安全版本以保证性能，在多线程中使用线程安全版本.

### 25. Combiner 的组件需要注意什么

因为combiner在mapreduce过程中可能调用也肯能不调用，可能调一次也可能调多次，无法确定和控制

所以,combiner使用的原则是：有或没有都不能影响业务逻辑，使不使用combiner都不能影响最终reducer的结果。而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来。因为有时使用combiner不当的话会对统计结果造成错误的结局，还不如不用。比如对所有数求平均数：

Mapper端使用combiner

3 5 7 ->(3+5+7)/3=5

2 6 ->(2+6)/2=4

Reducer

(5+4)/2=9/2 不等于(3+5+7+2+6)/5=23/5

### 26. Storm 和sparkstreaming 之间的对比

Storm

1) 真正意义上的实时处理。（实时性）

2）想实现一些复杂的功能，比较麻烦，比如：实现滑动窗口 （易用性）

原生的API：spout bolt bolt

Trident框架：使用起来难度还是有一些。

3）没有一个完整的生态

**SparkStreaming**

1）有批处理的感觉，一次处理的数据量较小，然后基于内存很快就可以运行完成。相当于是准实时。 （实时性）

2）封装了很多高级的API，在用户去实现一个复杂的功能的时候，很容易就可以实现。 （易用性）

3）有完整的生态系统。同时可以配置SparkCore,SparkSQL，Mlib,GraphX等，他们之间可以实现无缝的切换。

做一个比喻来说明这两个的区别：

Storm就像是超市里面的电动扶梯，实时的都在运行；

SparkStreaming就像是超市里面的电梯，每次载一批人。

### 27. Hdp 在进行容灾性测试时，会出现什么问题吗

Ambari Server 是存在单点问题的，如果 Server 机器宕机了，就无法恢复整个 Ambari Server 的数据，也就是说无法再通过 Ambari 管理集群。

### 28. Kafka的数据流读取速度快的原因是什么，为什么选择kafka，而不是别的消息中间件

生产者（写入数据）

生产者（producer）是负责向Kafka提交数据的，我们先分析这一部分。

Kafka会把收到的消息都写入到硬盘中，它绝对不会丢失数据。为了优化写入速度Kafak采用了两个技术，顺序写入和MMFile。

顺序写入

因为硬盘是机械结构，每次读写都会寻址->写入，其中寻址是一个“机械动作”，它是最耗时的。所以硬盘最“讨厌”随机I/O，最喜欢顺序I/O。为了提高读写硬盘的速度，Kafka就是使用顺序I/O。

### 29. Spark 与hadoop 对比，有哪些优势

1）Spark相比Hadoop在处理模型上的优势

首先，Spark 摒弃了MapReduce 先 map 再 reduce这样的严格方式，Spark引擎执行更通用的有向无环图（DAG）算子。

另外，基于MR的计算引擎在shuffle过程中会将中间结果输出到磁盘上，进行存储和容错，而且HDFS的可靠机制是将文件存为3份。Spark是将执行模型抽象为通用的有向无环图执行计划(DAG),当到最后一步时才会进行计算，这样可以将多stage的任务串联或者并行执行，而无须将stage中间结果输出到HDFS。磁盘IO的性能明显低于内存，所以Hadoop的运行效率低于spark。

2）数据格式和内存布局

MR在读的模型处理方式上会引起较大的处理开销，spark抽象出弹性分布式数据集RDD，进行数据的存储。RDD能支持粗粒度写操作，但对于读取操作，RDD可以精确到每条记录，这使得RDD可以用来作为分布式索引。Spark的这些特性使得开发人员能够控制数据在不同节点上的不同分区，用户可以自定义分区策略，如hash分区等。

3）执行策略

MR在数据shuffle之前花费了大量的时间来排序，spark可减轻这个开销。因为spark任务在shuffle中不是所有的场合都需要排序，所以支持基于hash的分布式聚合，调度中采用更为通用的任务执行计划图(DAG)，每一轮次的输出结果都在内存中缓存。

4）任务调度的开销

传统的MR系统，是为了运行长达数小时的批量作业而设计的，在某些极端情况下，提交一个任务的延迟非常高。Spark采用了时间驱动的类库AKKA来启动任务，通过线程池复用线程来避免进程或线程启动和切换开销。

5）内存计算能力的扩展

spark的弹性分布式数据集（RDD）抽象使开发人员可以将处理流水线上的任何点持久化存储在跨越集群节点的内存中，来保证后续步骤需要相同数据集时就不必重新计算或从磁盘加载，大大提高了性能。这个特性使Spark 非常适合涉及大量迭代的算法，这些算法需要多次遍历相同数据集， 也适用于反应式（reactive）应用，这些应用需要扫描大量内存数据并快速响应用户的查询。

6）开发速度的提升

构建数据应用的最大瓶颈不是 CPU、磁盘或者网络，而是分析人员的生产率。所以spark通过将预处理到模型评价的整个流水线整合在一个编程环境中， 大大加速了开发过程。Spark 编程模型富有表达力，在 REPL 下包装了一组分析库，省去了多次往返 IDE 的开销。而这些开销对诸如 MapReduce 等框架来说是无法避免的。Spark 还避免了采样和从HDFS 来回倒腾数据所带来的问题，这些问题是 R 之类的框架经常遇到的。分析人员在数据上做实验的速度越快，他们能从数据中挖掘出价值的可能性就越大。

7）功能强大

作为一个通用的计算引擎，spark的核心 API 为数据转换提供了强大的基础，它独立于统计学、机器学习或矩阵代数的任何功能。而且它的 Scala 和 Python API 让我们可以用表达力极强的通用编程语言编写程序，还可以访问已有的库。

Spark 的内存缓存使它适应于微观和宏观两个层面的迭代计算。机器学习算法需要多次遍历训练集，可以将训练集缓存在内存里。在对数据集进行探索和初步了解时，数据科学家可以在运行查询的时候将数据集放在内存，也很容易将转换后的版本缓存起来，这样可以节省访问磁盘的开销。

### 30. Java hash冲突怎么解决

1）开放定址法：2）链地址法3、4）再哈希、建立公共溢出区



## 腾讯大数据题目

自我介绍
讲述HDFS上传文件和读文件的流程
HDFS在上传文件的时候，如果其中一个块突然损坏了怎么办
NameNode的作用
NameNode在启动的时候会做哪些操作
NameNode的HA
NameNode和DataNode之间有哪些操作
Innodb事务怎么实现的
项目介绍
Hadoop的作业提交流程
Hadoop怎么分片
如何减少Hadoop Map端到Reduce端的数据传输量
Hadoop的Shuffle
HMaster的作用
HBase的操作数据的步骤
Innodb的二进制文件和Redo日志的区别
Redo日志的格式(不知道这个)
二进制日志的复制(不知道这个)
题目来自：csdn leishenop

##############################
下面关于大数据的答案，个人见解，欢迎交流
讲述HDFS上传文件和读文件的流程

HDFS 上传流程   
过程解析：详解
这里描述的 是一个256M的文件上传过程 
① 由客户端 向 NameNode节点节点 发出请求
②NameNode 向Client返回可以可以存数据的 DataNode 这里遵循  机架感应  原则

③客户端 首先 根据返回的信息 先将 文件分块（Hadoop2.X版本 每一个block为 128M 而之前的版本为 64M）
④然后通过那么Node返回的DataNode信息 直接发送给DataNode 并且是 流式写入  同时 会复制到其他两台机器
⑤dataNode 向 Client通信 表示已经传完 数据块 同时向NameNode报告
⑥依照上面（④到⑤）的原理将 所有的数据块都上传结束 向 NameNode 报告 表明 已经传完所有的数据块 

这样 整个HDFS上传流程就 走完了 （来自csdn Only丶爱你）


相关文章：
HDFS文件读写及准确性介绍
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6966


Hadoop学习总结：HDFS读写过程解析
http://www.aboutyun.com/forum.php?mod=viewthread&tid=14846

HDFS追本溯源：租约，读写过程的容错处理及NN的主要数据结构
http://www.aboutyun.com/forum.php?mod=viewthread&tid=17620


HDFS在上传文件的时候，如果其中一个块突然损坏了怎么办
其中一个块坏了，只要有其它块存在，会自动检测还原。

NameNode的作用
namenode总体来说是管理和记录恢复功能。
比如管理datanode，保持心跳，如果超时则排除。
对于上传文件都有镜像images和edits,这些可以用来恢复。更多：
深度了解namenode---其 内部关键数据结构原理简介
http://www.aboutyun.com/forum.php?mod=viewthread&tid=7388


NameNode在启动的时候会做哪些操作
NameNode启动的时候，会加载fsimage

更多参考下面内容
NameNode启动过程fsimage加载过程

Fsimage加载过程完成的操作主要是为了：
1.         从fsimage中读取该HDFS中保存的每一个目录和每一个文件
2.         初始化每个目录和文件的元数据信息
3.         根据目录和文件的路径，构造出整个namespace在内存中的镜像
4.         如果是文件，则读取出该文件包含的所有blockid，并插入到BlocksMap中。
整个加载流程如下图所示：
  

如上图所示，namenode在加载fsimage过程其实非常简单，就是从fsimage中不停的顺序读取文件和目录的元数据信息，并在内存中构建整个namespace，同时将每个文件对应的blockid保存入BlocksMap中，此时BlocksMap中每个block对应的datanodes列表暂时为空。当fsimage加载完毕后，整个HDFS的目录结构在内存中就已经初始化完毕，所缺的就是每个文件对应的block对应的datanode列表信息。这些信息需要从datanode的blockReport中获取，所以加载fsimage完毕后，namenode进程进入rpc等待状态，等待所有的datanodes发送blockReports。



NameNode的HA
NameNode的HA一个备用，一个工作，且一个失败后，另一个被激活。他们通过journal node来实现共享数据。
更多
Hadoop之NameNode+ResourceManager高可用原理分析
http://www.aboutyun.com/forum.php?mod=viewthread&tid=16024

Hadoop常见 HA方案 及如何解决HA
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6724


NameNode和DataNode之间有哪些操作
这个问题有些歧义。操作具体可以查看hadoop命令，应该超不出命令汇总
Hadoop Shell命令字典（可收藏）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6983

hadoop高级命令详解
http://www.aboutyun.com/forum.php?mod=viewthread&tid=14829



Hadoop的作业提交流程
Hadoop2.x Yarn作业提交（客户端）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=9498
Hadoop2.x Yarn作业提交（服务端）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=9496

更多：
hadoop作业提交脚本分析（1）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6954

hadoop作业提交脚本分析（2）
http://www.aboutyun.com/forum.php?mod=viewthread&tid=6956


Hadoop怎么分片
如何让hadoop按文件分片
http://www.aboutyun.com/forum.php?mod=viewthread&tid=14549

Hadoop分块与分片
http://www.aboutyun.com/blog-5994-697.html

如何减少Hadoop Map端到Reduce端的数据传输量
减少传输量，可以让map处理完，让同台的reduce直接处理，理想情况下，没有数据传输。

Hadoop的Shuffle
彻底了解mapreduce核心Shuffle--解惑各种mapreduce问题
http://www.aboutyun.com/forum.php?mod=viewthread&tid=7078

hadoop代码笔记 Mapreduce shuffle过程之Map输出过程((1)
http://www.aboutyun.com/forum.php?mod=viewthread&tid=10335


HMaster的作用
hmaster的作用
为region server分配region.
负责region server的负载均衡。
发现失效的region server并重新分配其上的region.
Gfs上的垃圾文件回收。
处理schema更新请求。
更多
region server and hmaster server

HBase的操作数据的步骤
Hbase写数据，存数据，读数据的详细过程
http://www.aboutyun.com/forum.php?mod=viewthread&tid=10886


hadoop相关试题
MapTask并行机度是由什么决定的？ 
由切片数量决定的。
MR是干什么的？ 
MR将用户编写的业务逻辑代码和自带的默认组件结合起来组成一个完整的分布式应用程序放到hadoop集群上运行。
MR的实例进程： 
driver(mr的job提交客户端) 
MRAppMaster 
MapTask 
ReduceTask
combiner和partition的作用： 
combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量 
partition的默认实现是hashpartition，是map端将数据按照reduce个数取余，进行分区，不同的reduce来copy自己的数据。 
partition的作用是将数据分到不同的reduce进行计算，加快计算效果。
什么是shuffle 
map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle； 
shuffle: 洗牌、发牌——（核心机制：数据分区，排序，缓存）； 
具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序；
MR原理(详细解释参照：MR运行原理剖析)： 
InputFormat来读取数据，按行读取，返回KV值传到map方法中， 
context.write方法将处理后数据输出到outputCollector中， 
当outputCollector中的数据累计到一定数量后再将数据传到内存的环形缓冲区做处理， 
当环形缓冲区中的数据累积到一定数量后再将数据通过Splier多次溢出到本地磁盘的多个文件中，期间会对各个溢出的数据进行分区、排序 
然后对多个文件进行merge（归并排序）形成一个输出结果大文件 
ruduceTask根据自己的分区号去各个mapTask机器上取输出结果文件 
将得到的各个结果文件进行merge，然后进入reduce阶段， 
context.write将最终结果输出到outPutformat上，进而输出到本地文件中。
举一个简单的例子说明mapreduce是怎么来运行的 ? 
wd例子。详细解释参考：Wd详解
什么是yarn？ 
Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序。
namenode的safemode是怎么回事？如何才能退出safemode？ 
namenode在刚启动的时候元数据只有文件块信息，没有文件所在datanode的信息，需要datanode自己向namenode汇报。如果namenode发现datanode汇报的文件块信息没有达到namenode内存中所有文件块的总阈值的一个百分比，namenode就会处于safemode。 
只有达到这个阈值，namenode才会推出safemode。也可手动强制退出。

secondarynamenode的主要职责是什么？简述其工作机制 
sn的主要职责是执行checkpoint操作 
每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）

如果namenode宕机，datanode节点是否也会跟着挂掉？ 
否
一个datanode 宕机,怎么一个流程恢复？ 
Datanode宕机了后，如果是短暂的宕机，可以实现写好脚本监控，将它启动起来。如果是长时间宕机了，那么datanode上的数据应该已经被备份到其他机器了， 
那这台datanode就是一台新的datanode了，删除他的所有数据文件和状态文件，重新启动
hadoop 的 namenode 宕机,怎么解决？ 
先分析宕机后的损失，宕机后直接导致client无法访问，内存中的元数据丢失，但是硬盘中的元数据应该还存在，如果只是节点挂了， 
重启即可，如果是机器挂了，重启机器后看节点是否能重启，不能重启就要找到原因修复了。 
但是最终的解决方案应该是在设计集群的初期就考虑到这个问题，做namenode的HA。
简述hadoop安装

改IP，修改Host文件； 
装JDK配置环境变量； 
装Hadoop配置环境变量； 
修改hadoop的配置文件如core-site、marp-site、yarn-site、dfs-site等； 
namenode进行格式化； 
start-all；

请列出hadoop正常工作时要启动那些进程，并写出各自的作用。 
namenode:管理集群并记录datanode的元数据，相应客户端的请求。 
seconder namenode：对namenode一定范围内的数据做一份快照性备份。 
datanode：存储数据。 
jobTracker：管理客户端提交的任务，并将任务分配给TaskTracker。 
TaskTracker：执行各个Task。

JobTracker和TaskTracker的功能 
JobTracker是一个master服务，软件启动之后JobTracker接收Job，负责调度Job的每一个子任务task运行于TaskTracker上， 
并监控它们，如果发现有失败的task就重新运行它。一般情况应该把JobTracker部署在单独的机器上。 
TaskTracker是运行在多个节点上的slaver服务。TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务。

用mapreduce怎么处理数据倾斜问题？ 
数据倾斜：map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长， 
这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多， 
从而导致某几个节点迟迟运行不完，此称之为数据倾斜。 
解决：自己实现partition类，用key和value相加取hash值。

Mapreduce 的 map 数量 和 reduce 数量 怎么确定 ,怎么配置？ 
map的数量有数据块决定，reduce数量随便配置。

hdfs的体系结构 
hdfs有namenode、secondraynamenode、datanode组成。 
namenode负责管理datanode和记录元数据 
secondraynamenode负责合并日志 
datanode负责存储数据

说下对hadoop 的一些理解,包括哪些组件 
详谈hadoop的应用，包括的组件分为三类，分别说明hdfs，yarn，mapreduce。

一些传统的hadoop 问题,mapreduce 他就问shuffle 阶段,你怎么理解的 
Shuffle意义在于将不同map处理后的数据进行合理分配，让reduce处理，从而产生了排序、分区。

NameNode 负责管理 metadata，client 端每次读写请求，它都会从磁盘中读取或则会写入 metadata信息并反馈client 端。（错误） 
修改后分析： 
NameNode 不需要从磁盘读取 metadata，所有数据都在内存中，硬盘上的只是序列化的结果，只有每次 
namenode 启动的时候才会读取。

Hive相关试题
你的数据库是不是很大么,有没有分表,分区,你是怎么实现的 
hive内部表和外部表的区别 
内部表：加载数据到hive所在的hdfs目录，删除时，元数据和数据文件都删除 
外部表：不加载数据到hive所在的hdfs目录，删除时，只删除表结构。

分桶的作用 
最大的作用是提高join的效率。（1）获得更高的查询处理效率。（2）使取样（sampling）更高效。

Hive 你们用的是外部表还是内部表,有没有写过UDF。 
UDF： 
1、写对应的java代码自定义函数的逻辑 
2、将代码打成jar包上传到hive 
3、在hive创建临时函数与对应的class类相关联 
4、在hive中调用临时函数。

Hbase相关试题
hbase的rowkey怎么创建好？列族怎么创建比较好？（重点） 
hbase存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分利用排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性) 
一个列族在数据底层是一个文件，所以将经常一起查询的列放到一个列族中，列族尽量少，减少文件的寻址时间。
Redis,传统数据库,hbase,hive 每个之间的区别？(问的非常细) 
redis：分布式缓存，强调缓存，内存中数据 
传统数据库：注重关系 
hbase：列式数据库，无法做关系数据库的主外键，用于存储海量数据，底层基于hdfs 
hive：数据仓库工具，底层是mapreduce。不是数据库，不能用来做用户的交互存储
hdfs 和 hbase 各自使用场景。 
整理总结： 
首先一点需要明白：Hbase 是基于 HDFS 来存储的。 
HDFS： 
1、一次性写入，多次读取。 
2、保证数据的一致性。 
3、主要是可以部署在许多廉价机器中，通过多副本提高可靠性，提供了容错和恢复机制。 
Hbase： 
1、瞬间写入量很大，数据库不好支撑或需要很高成本支撑的场景。 
2、数据需要长久保存，且量会持久增长到比较大的场景 
3、hbase 不适用与有 join，多级索引，表关系复杂的数据模型 
4、大数据量 （100s TB 级数据） 且有快速随机访问的需求。 
如：淘宝的交易历史记录。数据量巨大无容置疑，面向普通用户的请求必然要即时响应。 
5、容量的优雅扩展 
大数据的驱使，动态扩展系统容量的必须的。例如：webPage DB。 
6、业务场景简单，不需要关系数据库中很多特性（例如交叉列、交叉表，事务，连接等等） 
7、优化方面：合理设计 rowkey。因为 hbase 的查
Storm相关试题
公司技术选型可能利用storm 进行实时计算,讲解一下storm 
描述下storm的设计模式，是基于work、excutor、task的方式运行代码，由spout、bolt组成等等。
实时流式计算框架,几个人,多长时间,细节问题,包括讲flume ,kafka ,storm 
的各个的组件组成,你负责那一块,如果需要你搭建你可以完成么?（多次提到）
Spark相关试题
你觉得spark 可以完全替代hadoop 么? 
spark会替代mr,不会代替yarn和hdfs.
公司之后倾向用spark 开发,你会么(就用java代码去写) 
会，spark使用scala开发的，在scala中可以随意使用jdk的类库，可以用java开发，但是最好用原生的scala开发，兼容性好，scala更灵活。
Java基础试题
请参考 
http://blog.csdn.net/qq_16633405/article/details/79211002



2.hadoop 的 namenode 宕机,怎么解决 
先分析宕机后的损失，宕机后直接导致client无法访问，内存中的元数据丢失，但是硬盘中的元数据应该还存在，如果只是节点挂了， 
重启即可，如果是机器挂了，重启机器后看节点是否能重启，不能重启就要找到原因修复了。但是最终的解决方案应该是在设计集群的初期 
就考虑到这个问题，做namenode的HA。 
3.一个datanode 宕机,怎么一个流程恢复 
Datanode宕机了后，如果是短暂的宕机，可以实现写好脚本监控，将它启动起来。如果是长时间宕机了，那么datanode上的数据应该已经 
被备份到其他机器了，那这台datanode就是一台新的datanode了，删除他的所有数据文件和状态文件，重新启动。 
4.Hbase 的特性,以及你怎么去设计 rowkey 和 columnFamily ,怎么去建一个table 
因为hbase是列式数据库，列非表schema的一部分，所以在设计初期只需要考虑rowkey 和 columnFamily即可，rowkey有位置相关性，所以 
如果数据是练习查询的，最好对同类数据加一个前缀，而每个columnFamily实际上在底层是一个文件，那么文件越小，查询越快，所以讲经 
常一起查询的列设计到一个列簇，但是列簇不宜过多。 
5.Redis,传统数据库,hbase,hive 每个之间的区别(问的非常细) 
Redis是缓存，围绕着内存和缓存说 
Hbase是列式数据库，存在hdfs上，围绕着数据量来说 
Hive是数据仓库，是用来分析数据的，不是增删改查数据的。 
6.公司之后倾向用spark 开发,你会么(就用java代码去写) 
会，spark使用scala开发的，在scala中可以随意使用jdk的类库，可以用java开发，但是最好用原生的scala开发，兼容性好，scala更灵活。

面试问题: 
1.笔试: java基础(基本全忘,做的很烂,复习大数据连单例都忘了怎么写) 
复习java面试宝典 
2.开始介绍项目,直接用大数据项目介绍,项目经理也懂大数据 
3.Mapreduce 一些流程,经过哪些步骤 
Map—combiner—partition—sort—copy—sort—grouping—reduce 
4.说下对hadoop 的一些理解,包括哪些组件 
详谈hadoop的应用，包括的组件分为三类，分别说明hdfs，yarn，mapreduce 
5.详细讲解下你流式实时计算的项目部署以及收集的结果情况 
讲解storm集群的部署方案，项目的大小，使用的worker数，数据收集在hbase或者hdfs，好处是什么 
6.你的数据库是不是很大么,有没有分表,分区,你是怎么实现的 
数据库的分表在设计初期是按照月份进行拆分的，不同的月份查询不同的表。分区没弄过。 
7.开始问java的一些东西(从各种框架原理到各种复杂SQL) 
8.多线程,并发,垃圾回收机制,数据结构(问这些,基本觉得看你是不是高级程序员了) 
多线程要知道操作方式，线程安全的锁，并且要知道lock锁 
垃圾回收机制需要详细了解（见云笔记），主要从内存划分，垃圾回收主要的工作区域，垃圾回收器的种类，各有什么优缺点， 
用在哪里合适。 
数据结构基本的要知道，复杂的参考相关的书籍。
面试问题: 
1.BI小组的3个年轻学生一起技术面试(一个是南开博士） 
2.数据量多少,集群规模多大,型号 
一般中型的电商或者互联网企业，日志量每天在200-500M左右，集群规模在30-50台左右，机器一般为dell的2000左右的服务器，型号不定 
大型的互联网公司据网上资料显示，日志量在GP-PB不等，集群规模在500-4000不等，甚至更多，机器型号不确定。 
3.项目,mapreduce 
介绍整个mapreduce项目流程，数据采集—数据聚合—数据分析—数据展示等 
4.实时流式计算框架,几个人,多长时间,细节问题,包括讲flume ,kafka ,storm 的各个的组件组成,你负责那一块,如果需要你搭建你可以 
完成么? 
5.你觉得spark 可以完全替代hadoop 么?
面试问题: 
1.一些传统的hadoop 问题,mapreduce 他就问shuffle 阶段,你怎么理解的 
Shuffle意义在于将不同map处理后的数据进行合理分配，让reduce处理，从而产生了排序、分区。 
2.Mapreduce 的 map 数量 和 reduce 数量 怎么确定 ,怎么配置 
Map无法配置，reduce随便配置 
3.唯一难住我的是他说实时计算,storm 如果碰上了复杂逻辑,需要算很长的时间,你怎么去优化 
拆分复杂的业务到多个bolt中，这样可以利用bolt的tree将速度提升 
4.Hive 你们用的是外部表还是内部表,有没有写过UDF(当然吹自己写过了),hive 的版本 
外部表，udf，udaf等，hive版本为1.0 
5.Hadoop 的版本 
如果是1.0版本就说1.2，如果是2.0版本，就说2.6或者2.7 
1.2为官方稳定版本，2.7为官方稳定版本。 
Apache Hadoop 2.7.1于美国时间2015年07月06日正式发布，本版本属于稳定版本，是自Hadoop 2.6.0以来又一个稳定版，同时也是 
Hadoop 2.7.x版本线的第一个稳定版本，也是 2.7版本线的维护版本，变化不大，主要是修复了一些比较严重的Bug 
6.实时流式计算的结果内容有哪些,你们需要统计出来么(我就说highchart展示) 
简单介绍日志监控、风控等结果内容，统计出来显示在报表或者邮件中。 
7.开始问java相关,包括luecne,solr(倒排索引的原理),框架呀,redis呀
京东商城 - 大数据
（1）Java篇 
1、JVM，GC（算法，新生代，老年代），JVM结构 
2、hashcode，hashMap，list，hashSet，equals（结构原理），A extends B（类的加载顺序） 
1.父类静态代码块； 
2.子类静态代码块； 
3.父类非静态代码块； 
4.父类构造函数； 
5.子类非静态代码块； 
6.子类构造函数； 
3、多线程，主线程，次线程，唤醒，睡眠 
略 
4、常见算法：冒泡算法，排序算法，二分查找，时间复杂度 
略 
（2）Flume篇 
1、数据怎么采集到Kafka，实现方式 
使用官方提供的flumeKafka插件，插件的实现方式是自定义了flume的sink，将数据从channle中取出，通过kafka的producer写入到kafka中， 
可以自定义分区等。 
2、flume管道内存，flume宕机了数据丢失怎么解决 
1、Flume的channel分为很多种，可以将数据写入到文件 
2、防止非首个agent宕机的方法数可以做集群或者主备 
3、flume配置方式，flume集群（问的很详细） 
Flume的配置围绕着source、channel、sink叙述，flume的集群是做在agent上的，而非机器上。 
4、flume不采集Nginx日志，通过Logger4j采集日志，优缺点是什么？ 
优点：Nginx的日志格式是固定的，但是缺少sessionid，通过logger4j采集的日志是带有sessionid的，而session可以通过redis共享， 
保证了集群日志中的同一session落到不同的tomcat时，sessionId还是一样的，而且logger4j的方式比较稳定，不会宕机。 
缺点：不够灵活，logger4j的方式和项目结合过于紧密，而flume的方式比较灵活，拔插式比较好，不会影响项目性能。 
5、flume和kafka采集日志区别，采集日志时中间停了，怎么记录之前的日志。 
Flume采集日志是通过流的方式直接将日志收集到存储层，而kafka试讲日志缓存在kafka集群，待后期可以采集到存储层。 
Flume采集中间停了，可以采用文件的方式记录之前的日志，而kafka是采用offset的方式记录之前的日志。 
（3）Kafka篇 
1、容错机制 
分区备份，存在主备partition 
2、同一topic不同partition分区 
？？？？ 
3、kafka数据流向 
Producer leader partition follower partition(半数以上) consumer 
4、kafka+spark-streaming结合丢数据怎么解决？ 
spark streaming从1.2开始提供了数据的零丢失，想享受这个特性，需要满足如下条件：

数据输入需要可靠的sources和可靠的receivers

应用metadata必须通过应用driver checkpoint

WAL（write ahead log）

1.1. 可靠的sources和receivers

spark streaming可以通过多种方式作为数据sources（包括kafka），输入数据通过receivers接收，通过replication存储于spark中（为了faultolerance，默认复制到两个spark executors），如果数据复制完成，receivers可以知道（例如kafka中更新offsets到zookeeper中）。这样当receivers在接收数据过程中crash掉，不会有数据丢失，receivers没有复制的数据，当receiver恢复后重新接收。

1.2. metadata checkpoint

可靠的sources和receivers，可以使数据在receivers失败后恢复，然而在driver失败后恢复是比较复杂的，一种方法是通过checkpoint metadata到HDFS或者S3。metadata包括：

· configuration

· code

· 一些排队等待处理但没有完成的RDD（仅仅是metadata，而不是data）

这样当driver失败时，可以通过metadata checkpoint，重构应用程序并知道执行到那个地方。

1.3. 数据可能丢失的场景

可靠的sources和receivers，以及metadata checkpoint也不可以保证数据的不丢失，例如：

· 两个executor得到计算数据，并保存在他们的内存中

· receivers知道数据已经输入

· executors开始计算数据

· driver突然失败

· driver失败，那么executors都会被kill掉

· 因为executor被kill掉，那么他们内存中得数据都会丢失，但是这些数据不再被处理

· executor中的数据不可恢复

1.4. WAL

为了避免上面情景的出现，spark streaming 1.2引入了WAL。所有接收的数据通过receivers写入HDFS或者S3中checkpoint目录，这样当driver失败后，executor中数据丢失后，可以通过checkpoint恢复。

1.5. At-Least-Once

尽管WAL可以保证数据零丢失，但是不能保证exactly-once，例如下面场景：

· Receivers接收完数据并保存到HDFS或S3

· 在更新offset前，receivers失败了

· Spark Streaming以为数据接收成功，但是Kafka以为数据没有接收成功，因为offset没有更新到zookeeper

· 随后receiver恢复了

· 从WAL可以读取的数据重新消费一次，因为使用的kafka High-Level消费API，从zookeeper中保存的offsets开始消费

1.6. WAL的缺点

通过上面描述，WAL有两个缺点：

· 降低了receivers的性能，因为数据还要存储到HDFS等分布式文件系统

· 对于一些resources，可能存在重复的数据，比如Kafka，在Kafka中存在一份数据，在Spark Streaming也存在一份（以WAL的形式存储在hadoop API兼容的文件系统中）

1.7. Kafka direct API

为了WAL的性能损失和exactly-once，spark streaming1.3中使用Kafka direct API。非常巧妙，Spark driver计算下个batch的offsets，指导executor消费对应的topics和partitions。消费Kafka消息，就像消费文件系统文件一样。

不再需要kafka receivers，executor直接通过Kafka API消费数据

WAL不再需要，如果从失败恢复，可以重新消费

exactly-once得到了保证，不会再从WAL中重复读取数据

1.8. 总结

主要说的是spark streaming通过各种方式来保证数据不丢失，并保证exactly-once，每个版本都是spark streaming越来越稳定，越来越向生产环境使用发展。

5、kafka中存储目录data/dir…..topic1和topic2怎么存储的，存储结构，data…..目录下有多少个分区，每个分区的存储格式是什么样的？ 
1、topic是按照“主题名-分区”存储的 
2、分区个数由配置文件决定 
3、每个分区下最重要的两个文件是0000000000.log和000000.index，0000000.log以默认1G大小回滚。 
（4）Hive篇 
1、hive partition分区 
分区表，动态分区 
2、insert into 和 override write区别？ 
insert into：将某一张表中的数据写到另一张表中 
override write：覆盖之前的内容。 
3、假如一个分区的数据主部错误怎么通过hivesql删除hdfs 
alter table ptable drop partition (daytime=’20140911’,city=’bj’); 
元数据，数据文件都删除，但目录daytime= 20140911还在 
（5）Storm篇 
1、开发流程，容错机制 
开发流程： 
1、写主类（设计spout和bolt的分发机制） 
2、写spout收集数据 
3、写bolt处理数据，根据数据量和业务的复杂程度，设计并行度。 
容错机制：采用ack和fail进行容错，失败的数据重新发送。 
2、storm和spark-streaming：为什么用storm不同spark-streaming 
3、mr和spark区别，怎么理解spark-rdd 
Mr是文件方式的分布式计算框架，是将中间结果和最终结果记录在文件中，map和reduce的数据分发也是在文件中。 
spark是内存迭代式的计算框架，计算的中间结果可以缓存内存，也可以缓存硬盘，但是不是每一步计算都需要缓存的。 
Spark-rdd是一个数据的分区记录集合……………… 
4、sqoop命令

sqoop import –connect jdbc:mysql://192.168.56.204:3306/sqoop –username hive –password hive –table jobinfo –target-dir /sqoop/test7 –inline-lob-limit 16777216 –fields-terminated-by ‘\t’ -m 2

sqoop create-hive-table –connect jdbc:mysql://192.168.56.204:3306/sqoop –table jobinfo –username hive –password hive –hive-table sqtest –fields-terminated-by “\t” –lines-terminated-by “\n”;

（6）Redis篇 
1、基本操作，存储格式 
略 
（7）Mysql篇 
1、mysql集群的分布式事务 
京东自主开发分布式MYSQL集群系统 
2、mysql性能优化（数据方面） 
数据的分表、分库、分区 
（6）Hadoop篇 
1、hadoop HA 两个namenode和zk之间的通信，zk的选举机制？ 
HA是通过先后获取zk的锁决定谁是主 
Zk的选举机制，涉及到全新机群的选主和数据恢复的选主 
2、mr运行机制

3、yarn流程

1) 用户向YARN 中提交应用程序， 其中包括ApplicationMaster 程序、启动ApplicationMaster 的命令、用户程序等。 
2) ResourceManager 为该应用程序分配第一个Container， 并与对应的NodeManager 通信，要求它在这个Container 中启动应用程序 
的ApplicationMaster。 
3) ApplicationMaster 首先向ResourceManager 注册， 这样用户可以直接通过ResourceManage 查看应用程序的运行状态，然后它将 
为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。 
4) ApplicationMaster 采用轮询的方式通过RPC 协议向ResourceManager 申请和领取资源。 
5) 一旦ApplicationMaster 申请到资源后，便与对应的NodeManager 通信，要求它启动任务。 
6) NodeManager 为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行 
该脚本启动任务。 
7) 各个任务通过某个RPC 协议向ApplicationMaster 汇报自己的状态和进度，以让ApplicationMaster 随时掌握各个任务的运行状态， 
从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC 向ApplicationMaster 查询应用程序的当前运行状态。 
8) 应用程序运行完成后，ApplicationMaster 向ResourceManager 注销并关闭自己。 
（7）Hbase 
1、涉及到概念，文档 
（8）Spark篇 
1、spark原理

Spark应用转换流程

1、 spark应用提交后，经历了一系列的转换，最后成为task在每个节点上执行

2、 RDD的Action算子触发Job的提交，生成RDD DAG

3、 由DAGScheduler将RDD DAG转化为Stage DAG，每个Stage中产生相应的Task集合

4、 TaskScheduler将任务分发到Executor执行

5、 每个任务对应相应的一个数据块，只用用户定义的函数处理数据块

Driver运行在Worker上

通过org.apache.spark.deploy.Client类执行作业，作业运行命令如下：

作业执行流程描述：

1、客户端提交作业给Master

2、Master让一个Worker启动Driver，即SchedulerBackend。Worker创建一个DriverRunner线程，DriverRunner启动SchedulerBackend进程。

3、另外Master还会让其余Worker启动Exeuctor，即ExecutorBackend。Worker创建一个ExecutorRunner线程，ExecutorRunner会启动ExecutorBackend进程。

4、ExecutorBackend启动后会向Driver的SchedulerBackend注册。SchedulerBackend进程中包含DAGScheduler，它会根据用户程序，生成执行计划，并调度执行。对于每个stage的task，都会被存放到TaskScheduler中，ExecutorBackend向SchedulerBackend汇报的时候把TaskScheduler中的task调度到ExecutorBackend执行。

5、所有stage都完成后作业结束。

Driver运行在客户端

作业执行流程描述：

1、客户端启动后直接运行用户程序，启动Driver相关的工作：DAGScheduler和BlockManagerMaster等。

2、客户端的Driver向Master注册。

3、Master还会让Worker启动Exeuctor。Worker创建一个ExecutorRunner线程，ExecutorRunner会启动ExecutorBackend进程。

4、ExecutorBackend启动后会向Driver的SchedulerBackend注册。Driver的DAGScheduler解析作业并生成相应的Stage，每个Stage包含的Task通过TaskScheduler分配给Executor执行。

5、所有stage都完成后作业结束。


1.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?

　　假如每个url大小为10bytes，那么可以估计每个文件的大小为50G×64=320G，远远大于内存限制的4G，所以不可能将其完全加载到内存中处理，可以采用分治的思想来解决。

　　Step1：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,...,a999，每个小文件约300M);

　　Step2:遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,...,b999);

　　巧妙之处：这样处理后，所有可能相同的url都被保存在对应的小文件(a0vsb0,a1vsb1,...,a999vsb999)中，不对应的小文件不可能有相同的url。然后我们只要求出这个1000对小文件中相同的url即可。

　　Step3：求每对小文件ai和bi中相同的url时，可以把ai的url存储到hash_set/hash_map中。然后遍历bi的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。

　　草图如下(左边分解A，右边分解B，中间求解相同url)：



2.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，要求返回频数最高的100个词。

　　Step1：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件(记为f0,f1,...,f4999)中，这样每个文件大概是200k左右，如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M;

　　Step2：对每个小文件，统计每个文件中出现的词以及相应的频率(可以采用trie树/hash_map等)，并取出出现频率最大的100个词(可以用含100个结点的最小堆)，并把100词及相应的频率存入文件，这样又得到了5000个文件;

　　Step3：把这5000个文件进行归并(类似与归并排序);

　　草图如下(分割大问题，求解小问题，归并)：



3.现有海量日志数据保存在一个超级大的文件中，该文件无法直接读入内存，要求从中提取某天出访问百度次数最多的那个IP。

　　Step1：从这一天的日志数据中把访问百度的IP取出来，逐个写入到一个大文件中;

　　Step2：注意到IP是32位的，最多有2^32个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件;

　　Step3：找出每个小文中出现频率最大的IP(可以采用hash_map进行频率统计，然后再找出频率最大的几个)及相应的频率;

　　Step4：在这1000个最大的IP中，找出那个频率最大的IP，即为所求。

　　草图如下：



4.LVS和HAProxy相比，它的缺点是什么?

　　之前，的确是用LVS进行过MySQL集群的负载均衡，对HAProxy也有过了解，但是将这两者放在眼前进行比较，还真没试着了解过。面试中出现了这么一题，面试官给予的答案是LVS的配置相当繁琐，后来查找了相关资料，对这两种负载均衡方案有了更进一步的了解。LVS的负载均衡性能之强悍已经达到硬件负载均衡的F5的百分之60了，而HAproxy的负载均衡和Nginx负载均衡，均为硬件负载均衡的百分之十左右。由此可见，配置复杂，相应的效果也是显而易见的。在查找资料的过程中，试着将LVS的10种调度算法了解了一下，看似数量挺多的10种算法其实在不同的算法之间，有些只是有着一些细微的差别。在这10种调度算法中，静态调度算法有四种，动态调度算法有6种。


静态调度算法：

　　①RR轮询调度算法

　　这种调度算法不考虑服务器的状态，所以是无状态的，同时也不考虑每个服务器的性能，比如我有1-N台服务器，来N个请求了，第一个请求给第一台，第二个请求给第二台，，，第N个请求给第N台服务器，就酱紫。

　　②加权轮询

　　这种调度算法是考虑到服务器的性能的，你可以根据不同服务器的性能，加上权重进行分配相应的请求。

　　③基于目的地址的hash散列

　　这种调度算法和基于源地址的hash散列异曲同工，都是为了维持一个session，基于目的地址的hash散列，将记住同一请求的目的地址，将这类请求发往同一台目的服务器。简而言之，就是发往这个目的地址的请求都发往同一台服务器。而基于源地址的hash散列，就是来自同一源地址的请求都发往同一台服务器。

　　④基于源地址的hash散列

　　上述已讲，不再赘述。

动态调度

　　①最少连接调度算法

　　这种调度算法会记录响应请求的服务器上所建立的连接数，每接收到一个请求会相应的将该服务器的所建立连接数加1，同时将新来的请求分配到当前连接数最少的那台机器上。

　　②加权最少连接调度算法

　　这种调度算法在最少连接调度算法的基础上考虑到服务器的性能。当然，做这样子的考虑是有其合理性存在的，如果是同一规格的服务器，那么建立的连接数越多，必然越增加其负载，那么仅仅根据最少连接数的调度算法，必然可以实现合理的负载均衡。但如果，服务器的性能不一样呢?比如我有一台服务器，最多只能处理10个连接，现在建立了3个，还有一台服务器最多能处理1000条连接，现在建立了5个，如果单纯地按照上述的最少连接调度算法，妥妥的前者嘛，但前者已经建立了百分之三十的连接了，而后者连百分之一的连接还没有建立，试问，这合理吗?显然不合理。所以加上权重，才算合理。相应的公式也相当简单：active*256/weight。

　　③最短期望调度算法

　　这种算法，是避免出现上述加权最少连接调度算法中的一种特殊情况，导致即使加上权重，调度器也无差别对待了，举个栗子：

　　假设有三台服务器ABC，其当前所建立的连接数相应地为1,2,3，而权重也是1,2,3。那么如果按照加权最少连接调度算法的话，算出来是这样子的：

　　A:1256/1=256

　　B:2256/2=256

　　C:3256/3=256

　　我们会发现，即便加上权重，A、B、C，经过计算还是一样的，这样子调度器会无差别的在A、B、C中任选一台，将请求发过去。

　　而最短期望将active256/weight的算法改进为(active+1)256/weight

　　那么还是之前的例子：

　　A:(1+1)256/1=2/1256=2256

　　B:(2+1)256/2=3/2256=1.5256

　　C:(3+1)256、3=4/3256≈1.3256

　　显然C

　　④永不排队算法

　　将请求发给当前连接数为0的服务器上。

　　⑤基于局部的最少连接调度算法

　　这种调度算法应用于Cache系统，维持一个请求到一台服务器的映射，其实我们仔细想想哈，之前做的一系列最少连接相关的调度算法。考虑到的是服务器的状态与性能，但是一次请求并不是单向的，就像有一个从未合作过的大牛，他很闲，你让他去解决一个之前碰到过的一个问题，未必有找一个之前已经跟你合作过哪怕现在不怎么闲的臭皮匠效果好哦~，所以基于局部的最少连接调度算法，维持的这种映射的作用是，如果来了一个请求，相对应的映射的那台服务器，没有超载，ok交给老伙伴完事吧，俺放心，如果那台服务器不存在，或者是超载的状态且有其他服务器工作在一半的负载状态，则按最少连接调度算法在集群其余的服务器中找一台将请求分配给它。

　　⑥基于复制的局部最少连接调度算法

　　这种调度算法同样应用于cache系统，但它维持的不是到一台服务器的映射而是到一组服务器的映射，当有新的请求到来，根据最小连接原则，从该映射的服务器组中选择一台服务器，如果它没有超载则交给它去处理这个请求，如果发现它超载，则从服务器组外的集群中，按最少连接原则拉一台机器加入服务器组，并且在服务器组有一段时间未修改后，将最忙的那台服务器从服务器组中剔除。

5.Sqoop用起来感觉怎样?

　　说实话，Sqoop在导入数据的速度上确实十分感人，通过进一步了解，发现Sqoop1和Sqoop2在架构上还是有明显不同的，无论是从数据类型上还是从安全权限，密码暴露方面，Sqoop2都有了明显的改进，同时同一些其他的异构数据同步工具比较,如淘宝的DataX或者Kettle相比，Sqoop无论是从导入数据的效率上还是从支持插件的丰富程度上，Sqoop还是相当不错滴!!

6.ZooKeeper的角色以及相应的Zookepper工作原理?

　　果然，人的记忆力是有衰减曲线的，当面试官抛出这个问题后，前者角色，我只答出了两种(leader和follower)，后者原理压根就模糊至忘记了。所以恶补了一下，涉及到Zookeeper的角色大概有如下四种：leader、learner(follower)、observer、client。其中leader主要用来决策和调度，follower和observer的区别仅仅在于后者没有写的职能，但都有将client请求提交给leader的职能，而observer的出现是为了应对当投票压力过大这种情形的，client就是用来发起请求的。而Zookeeper所用的分布式一致性算法包括leader的选举其实和-原始部落的获得神器为酋长，或者得玉玺者为皇帝类似，谁id最小，谁为leader，会根据你所配置的相应的文件在相应的节点机下生成id，然后相应的节点会通过getchildren()这个函数获取之前设置的节点下生成的id，谁最小，谁是leader。并且如果万一这个leader挂掉了或者堕落了，则由次小的顶上。而且在配置相应的zookeeper文件的时候回有类似于如下字样的信息：Server.x=AAAA:BBBB:CCCC。其中的x即为你的节点号哈，AAAA对应你所部属zookeeper所在的ip地址，BBBB为接收client请求的端口，CCCC为重新选举leader端口。

7.HBase的Insert与Update的区别?

　　这个题目是就着最近的一次项目问的，当时实现的与hbase交互的三个方法分别为insert、delete、update。由于那个项目是对接的一个项目，对接的小伙伴和我协商了下，不将update合并为insert，如果合并的话，按那个项目本身，其实通过insert执行overwrite相当于间接地Update，本质上，或者说在展现上是没什么区别的包括所调用的put。但那仅仅是就着那个项目的程序而言，如果基于HBaseshell层面。将同一rowkey的数据插入HBase，其实虽然展现一条，但是相应的timestamp是不一样的，而且最大的版本数可以通过配置文件进行相应地设置。

8.请简述大数据的结果展现方式。

　　1)报表形式

　　基于数据挖掘得出的数据报表，包括数据表格、矩阵、图形和自定义格式的报表等，使用方便、设计灵活。

　　2)图形化展现

　　提供曲线、饼图、堆积图、仪表盘、鱼骨分析图等图形形式宏观展现模型数据的分布情况，从而便于进行决策。

　　3)KPI展现

　　提供表格式绩效一览表并可自定义绩效查看方式，如数据表格或走势图，企业管理者可根据可度量的目标快速评估进度。


　　4)查询展现


　　按数据查询条件和查询内容，以数据表格来汇总查询结果，提供明细查询功能，并可在查询的数据表格基础上进行上钻、下钻、旋转等操作。


　　9.例举身边的大数据。


　　i.QQ，微博等社交软件产生的数据


　　ii.天猫，京东等电子商务产生的数据


　　iii.互联网上的各种数据


　　10.简述大数据的数据管理方式。


　　答：对于图像、视频、URL、地理位置等类型多样的数据，难以用传统的结构化方式描述，因此需要使用由多维表组成的面向列存储的数据管理系统来组织和管理数据。也就是说，将数据按行排序，按列存储，将相同字段的数据作为一个列族来聚合存储。不同的列族对应数据的不同属性，这些属性可以根据需求动态增加，通过这样的分布式实时列式数据库对数据统一进行结构化存储和管理，避免了传统数据存储方式下的关联查询。


　　11.什么是大数据?


　　答：大数据是指无法在容许的时间内用常规软件工具对其内容进行抓取、管理和处理的数据。


　　12.海量日志数据，提取出某日访问百度次数最多的那个IP。


　　首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP(可以采用hash_map进行频率统计，然后再找出频率最大的几个)及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。


　　或者如下阐述(雪域之鹰)：


　　算法思想：分而治之+Hash


　　1)IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理;


　　2)可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址;


　　3)对于每一个小文件，可以构建一个IP为key，出现次数为value的Hashmap，同时记录当前出现次数最多的那个IP地址;


　　4)可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP;


　　13.搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。


　　假设目前有一千万个记录(这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。)，请你统计最热门的10个查询串，要求使用的内存不能超过1G。


　　典型的TopK算法，还是在这篇文章里头有所阐述，详情请参见：十一、从头到尾彻底解析Hash表算法。


　　文中，给出的最终算法是：


　　第一步、先对这批海量数据预处理，在O(N)的时间内用Hash表完成统计(之前写成了排序，特此订正。July、2011.04.27);


　　第二步、借助堆这个数据结构，找出TopK，时间复杂度为N‘logK。


　　即，借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比所以，我们最终的时间复杂度是：O(N)+N’*O(logK)，(N为1000万，N’为300万)。ok，更多，详情，请参考原文。


　　或者：采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。


　　14.有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。


　　方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件(记为x0,x1,…x4999)中。这样每个文件大概是200k左右。


　　如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。


　　对每个小文件，统计每个文件中出现的词以及相应的频率(可以采用trie树/hash_map等)，并取出出现频率最大的100个词(可以用含100个结点的最小堆)，并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并(类似与归并排序)的过程了。


　　15.有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。


　　还是典型的TOPK算法，解决方案如下：


　　方案1：


　　顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件(记为)中。这样新生成的文件每个的大小大约也1G(假设hash函数是随机的)。


　　找一台内存在2G左右的机器，依次对用hash_map(query,query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件(记为)。


　　对这10个文件进行归并排序(内排序与外排序相结合)。


　　方案2：


　　一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。


　　方案3：


　　与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理(比如MapReduce)，最后再进行合并。


　　16.给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?


　　方案1：可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。


　　遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,…,a999)中。这样每个小文件的大约为300M。


　　遍历文件b，采取和a相同的方式将url分别存储到1000小文件(记为b0,b1,…,b999)。这样处理后，所有可能相同的url都在对应的小文件(a0vsb0,a1vsb1,…,a999vsb999)中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。


　　求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。


　　方案2：如果允许有一定的错误率，可以使用Bloomfilter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloomfilter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloomfilter，如果是，那么该url应该是共同的url(注意会有一定的错误率)。


　　Bloomfilter日后会在本BLOG内详细阐述。


　　17.在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。


　　方案1：采用2-Bitmap(每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义)进行，共需内存2^32*2bit=1GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。


　　方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。


　　18.腾讯面试题：给40亿个不重复的unsignedint的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中?


　　与上第6题类似，我的第一反应时快速排序+二分查找。以下是其它更好的方法：


　　方案1：oo，申请512M的内存，一个bit位代表一个unsignedint值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。


　　dizengrong：


　　方案2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：


　　又因为2^32为40亿多，所以给定一个数可能在，也可能不在其中;


　　这里我们把40亿个数中的每一个用32位的二进制来表示


　　假设这40亿个数开始放在一个文件中。


　　然后将这40亿个数分成两类:


　　1.最高位为0


　　2.最高位为1


　　并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿(这相当于折半了);


　　与要查找的数的最高位比较并接着进入相应的文件再查找


　　再然后把这个文件为又分成两类:


　　1.次最高位为0


　　2.次最高位为1


　　并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿(这相当于折半了);


　　与要查找的数的次最高位比较并接着进入相应的文件再查找。


　　…….


　　以此类推，就可以找到了,而且时间复杂度为O(logn)，方案2完。


　　附：这里，再简单介绍下，位图方法：


　　使用位图法判断整形数组是否存在重复


　　判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。


　　位图法比较适合于这种情况，它的做法是按照集合中最大元素max创建一个长度为max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上1，如遇到5就给新数组的第六个元素置1，这样下次再遇到5想置位时发现新数组的第六个元素已经是1了，这说明这次的数据肯定和以前的数据存在着重复。这种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为2N。如果已知数组的最大值即能事先给新数组定长的话效率还能提高一倍。


　　欢迎，有更好的思路，或方法，共同交流。


　　19.怎么在海量数据中找出重复次数最多的一个?


　　方案1：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求(具体参考前面的题)。


　　20.上千万或上亿数据(有重复)，统计其中出现次数最多的钱N个数据。


　　方案1：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前N个出现次数最多的数据了，可以用第2题提到的堆机制完成。


　　21.一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。


　　方案1：这题是考虑时间效率。用trie树统计每个词出现的次数，时间复杂度是O(n*le)(le表示单词的平准长度)。然后是找出出现最频繁的前10个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是O(n*lg10)。所以总的时间复杂度，是O(n*le)与O(n*lg10)中较大的哪一个。


　　附、100w个数中找出最大的100个数。


　　方案1：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。


　　方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。


　　方案3：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。


　　第二部分、十个海量数据处理方法大总结


　　ok，看了上面这么多的面试题，是否有点头晕。是的，需要一个总结。接下来，本文将简单总结下一些处理海量数据问题的常见方法，而日后，本BLOG内会具体阐述这些方法。


　　一、Bloomfilter


　　适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集


　　基本原理及要点：


　　对于原理来说很简单，位数组+k个独立hash函数。将hash函数对应的值的位数组置1，查找时如果发现所有hash函数对应位都是1说明存在，很明显这个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是countingBloomfilter，用一个counter数组代替位数组，就可以支持删除了。


　　还有一个比较重要的问题，如何根据输入元素个数n，确定位数组m的大小及hash函数个数。当hash函数个数k=(ln2)*(m/n)时错误率最小。在错误率不大于E的情况下，m至少要等于n*lg(1/E)才能表示任意n个元素的集合。但m还应该更大些，因为还要保证bit数组里至少一半为0，则m应该>=nlg(1/E)*lge大概就是nlg(1/E)1.44倍(lg表示以2为底的对数)。


　　举个例子我们假设错误率为0.01，则此时m应大概是n的13倍。这样k大概是8个。


　　注意这里m与n的单位不同，m是bit为单位，而n则是以元素个数为单位(准确的说是不同元素的个数)。通常单个元素的长度都是有很多bit的。所以使用bloomfilter内存上通常都是节省的。


　　扩展：


　　Bloomfilter将集合中的元素映射到位数组中，用k(k为哈希函数个数)个映射位是否全1表示元素在不在这个集合中。Countingbloomfilter(CBF)将位数组中的每一位扩展为一个counter，从而支持了元素的删除操作。SpectralBloomFilter(SBF)将其与集合元素的出现次数关联。SBF采用counter中的最小值来近似表示元素的出现频率。


　　问题实例：给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢?


　　根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。


　　二、Hashing


　　适用范围：快速查找，删除的基本数据结构，通常需要总数据量可以放入内存


　　基本原理及要点：


　　hash函数选择，针对字符串，整数，排列，具体相应的hash方法。


　　碰撞处理，一种是openhashing，也称为拉链法;另一种就是closedhashing，也称开地址法，openedaddressing。


　　扩展：


　　d-lefthashing中的d是多个的意思，我们先简化这个问题，看一看2-lefthashing。2-lefthashing指的是将一个哈希表分成长度相等的两半，分别叫做T1和T2，给T1和T2分别配备一个哈希函数，h1和h2。在存储一个新的key时，同时用两个哈希函数进行计算，得出两个地址h1[key]和h2[key]。这时需要检查T1中的h1[key]位置和T2中的h2[key]位置，哪一个位置已经存储的(有碰撞的)key比较多，然后将新key存储在负载少的位置。如果两边一样多，比如两个位置都为空或者都存储了一个key，就把新key存储在左边的T1子表中，2-left也由此而来。在查找一个key时，必须进行两次hash，同时查找两个位置。


　　问题实例：


　　1).海量日志数据，提取出某日访问百度次数最多的那个IP。


　　IP的数目还是有限的，最多2^32个，所以可以考虑使用hash将ip直接存入内存，然后进行统计。


　　三、bit-map


　　适用范围：可进行数据的快速查找，判重，删除，一般来说数据范围是int的10倍以下


　　基本原理及要点：使用bit数组来表示某些元素是否存在，比如8位电话号码


　　扩展：bloomfilter可以看做是对bit-map的扩展


　　问题实例：


　　1)已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。


　　8位最多99999999，大概需要99m个bit，大概10几m字节的内存即可。


　　2)2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。


　　将bit-map扩展一下，用2bit表示一个数即可，0表示未出现，1表示出现一次，2表示出现2次及以上。或者我们不用2bit来进行表示，我们用两个bit-map即可模拟实现这个2bit-map。


　　四、堆


　　适用范围：海量数据前n大，并且n比较小，堆可以放入内存


　　基本原理及要点：最大堆求前n小，最小堆求前n大。方法，比如求前n小，我们比较当前元素与最大堆里的最大元素，如果它小于最大元素，则应该替换那个最大元素。这样最后得到的n个元素就是最小的n个。适合大数据量，求前n小，n的大小比较小的情况，这样可以扫描一遍即可得到所有的前n元素，效率很高。


　　扩展：双堆，一个最大堆与一个最小堆结合，可以用来维护中位数。


　　问题实例：


　　1)100w个数中找最大的前100个数。


　　用一个100个元素大小的最小堆即可。


　　五、双层桶划分—-其实本质上就是【分而治之】的思想，重在“分”的技巧上!


　　适用范围：第k大，中位数，不重复或重复的数字


　　基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。可以通过多次缩小，双层只是一个例子。


　　扩展：


　　问题实例：


　　1).2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。


　　有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。


　　2).5亿个int找它们的中位数。


　　这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。


　　实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用directaddrtable进行统计了。


　　六、数据库索引


　　适用范围：大数据量的增删改查


　　基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。


　　七、倒排索引(Invertedindex)


　　适用范围：搜索引擎，关键字查询


　　基本原理及要点：为何叫倒排索引?一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。


　　以英文为例，下面是要被索引的文本：


　　T0=“itiswhatitis”


　　T1=“whatisit”


　　T2=“itisabanana”


　　我们就能得到下面的反向文件索引：


　　“a”:{2}


　　“banana”:{2}


　　“is”:{0,1,2}


　　“it”:{0,1,2}


　　“what”:{0,1}


　　检索的条件”what”,”is”和”it”将对应集合的交集。


　　正向索引开发出来用来存储每个文档的单词的列表。正向索引的查询往往满足每个文档有序频繁的全文查询和每个单词在校验文档中的验证这样的查询。在正向索引中，文档占据了中心的位置，每个文档指向了一个它所包含的索引项的序列。也就是说文档指向了它包含的那些单词，而反向索引则是单词指向了包含它的文档，很容易看到这个反向的关系。


　　扩展：


　　问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。


　　八、外排序


　　适用范围：大数据的排序，去重


　　基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树


　　扩展：


　　问题实例：


　　1).有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。


　　这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1m做hash有些不够，所以可以用来排序。内存可以当输入缓冲区使用。


　　九、trie树


　　适用范围：数据量大，重复多，但是数据种类小可以放入内存


　　基本原理及要点：实现方式，节点孩子的表示方式


　　扩展：压缩实现。


　　问题实例：


　　1).有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。


　　2).1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现?


　　3).寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。


　　十、分布式处理mapreduce


　　适用范围：数据量大，但是数据种类小可以放入内存


　　基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。


　　扩展：


　　问题实例：


　　1).ThecanonicalexampleapplicationofMapReduceisaprocesstocounttheappearancesof


　　eachdifferentwordinasetofdocuments:


　　2).海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。


　　3).一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数(median)?


　　经典问题分析


　　上千万or亿数据(有重复)，统计其中出现次数最多的前N个数据,分两种情况：可一次读入内存，不可一次读入。


　　可用思路：trie树+堆，数据库索引，划分子集分别统计，hash，分布式计算，近似统计，外排序


　　所谓的是否能一次读入内存，实际上应该指去除重复后的数据量。如果去重后数据可以放入内存，我们可以为数据建立字典，比如通过map，hashmap，trie，然后直接进行统计即可。当然在更新每条数据的出现次数的时候，我们可以利用一个堆来维护出现次数最多的前N个数据，当然这样导致维护次数增加，不如完全统计后在求前N大效率高。


　　如果数据无法放入内存。一方面我们可以考虑上面的字典方法能否被改进以适应这种情形，可以做的改变就是将字典存放到硬盘上，而不是内存，这可以参考数据库的存储方法。


　　当然还有更好的方法，就是可以采用分布式计算，基本上就是map-reduce过程，首先可以根据数据值或者把数据hash(md5)后的值，将数据按照范围划分到不同的机子，最好可以让数据划分后可以一次读入内存，这样不同的机子负责处理各种的数值范围，实际上就是map。得到结果后，各个机子只需拿出各自的出现次数最多的前N个数据，然后汇总，选出所有的数据中出现次数最多的前N个数据，这实际上就是reduce过程。


　　实际上可能想直接将数据均分到不同的机子上进行处理，这样是无法得到正确的解的。因为一个数据可能被均分到不同的机子上，而另一个则可能完全聚集到一个机子上，同时还可能存在具有相同数目的数据。比如我们要找出现次数最多的前100个，我们将1000万的数据分布到10台机器上，找到每台出现次数最多的前100个，归并之后这样不能保证找到真正的第100个，因为比如出现次数最多的第100个可能有1万个，但是它被分到了10台机子，这样在每台上只有1千个，假设这些机子排名在1000个之前的那些都是单独分布在一台机子上的，比如有1001个，这样本来具有1万个的这个就会被淘汰，即使我们让每台机子选出出现次数最多的1000个再归并，仍然会出错，因为可能存在大量个数为1001个的发生聚集。因此不能将数据随便均分到不同机子上，而是要根据hash后的值将它们映射到不同的机子上处理，让不同的机器处理一个数值范围。


　　而外排序的方法会消耗大量的IO，效率不会很高。而上面的分布式方法，也可以用于单机版本，也就是将总的数据根据值的范围，划分成多个不同的子文件，然后逐个处理。处理完毕之后再对这些单词的及其出现频率进行一个归并。实际上就可以利用一个外排序的归并过程。


　　另外还可以考虑近似计算，也就是我们可以通过结合自然语言属性，只将那些真正实际中出现最多的那些词作为一个字典，使得这个规模可以放入内存。


　　【某公司笔试面试题】


　　1使用mr，spark,sparksql编写wordcount程序


　　【Spark版本】


　　valconf=newSparkConf().setAppName("wd").setMaster("local[1]")


　　valsc=newSparkContext(conf,2)


　　//加载


　　vallines=sc.textFile("tructField("name",DataTypes.StringType,true)")


　　valparis=lines.flatMap(line=>line.split("^A"))


　　valwords=paris.map((_,1))


　　valresult=words.reduceByKey(_+_).sortBy(x=>x._1,false)


　　//打印


　　result.foreach(


　　wds=>{


　　println("单词："+wds._1+"个数："+wds._2)


　　}


　　)


　　sc.stop()


　　【sparksql版本】


　　valconf=newSparkConf().setAppName("sqlWd").setMaster("local[1]")


　　valsc=newSparkContext(conf)


　　valsqlContext=newSQLContext(sc)


　　//加载


　　vallines=sqlContext.textFile("E:idea15createRecommederdatawords.txt")


　　valwords=lines.flatMap(x=>x.split("")).map(y=>Row(y))


　　valstructType=StructType(Array(StructField("name",DataTypes.StringType,true)))


　　valdf=sqlContext.createDataFrame(rows,structType)


　　df.registerTempTable("t_word_count")


　　sqlContext.udf.register("num_word",(name:String)=>1)


　　sqlContext.sql("selectname,num_word(name)fromt_word_count").groupBy(df.col("name")).count().show()


　　sc.stop()


　　2hive的使用，内外部表的区别，分区作用，UDF和Hive优化


　　(1)hive使用：仓库、工具


　　(2)hive内外部表：内部表数据永久删除，外部表数据删除后、其他人依然可以访问


　　(3)分区作用：防止数据倾斜


　　(4)UDF函数：用户自定义的函数(主要解决格式，计算问题)，需要继承UDF类


　　java代码实现


　　classTestUDFHiveextendsUDF{


　　publicStringevalute(Stringstr){


　　try{


　　return"hello"+str


　　}catch(Exceptione){


　　returnstr+"error"


　　}


　　}


　　}


　　(5)Hive优化：看做mapreduce处理


　　a排序优化：sortby效率高于orderby


　　b分区：使用静态分区(statu_date="20160516",location="beijin")，每个分区对应hdfs上的一个目录


　　c减少job和task数量：使用表链接操作


　　d解决groupby数据倾斜问题：设置hive.groupby.skewindata=true，那么hive会自动负载均衡


　　e小文件合并成大文件：表连接操作


　　f使用UDF或UDAF函数：hive中UDTF编写和使用(转) - ggjucheng - 博客园


　　3Hbase的rk设计，Hbase优化


　　aowkey:hbase三维存储中的关键(rowkey：行键，columnKey(family+quilaty)：列键，timestamp：时间戳)


　　owkey字典排序、越短越好


　　使用id+时间：9527+20160517使用hash散列：dsakjkdfuwdsf+9527+20160518


　　应用中，rowkey一般10~100bytes,8字节的整数倍，有利于提高操作系统性能


　　bHbase优化


　　分区：RegionSplit()方法NUMREGIONS=9


　　column不超过3个


　　硬盘配置，便于regionServer管理和数据备份及恢复


　　分配合适的内存给regionserver


　　其他：


　　hbase查询


　　(1)get


　　(2)scan


　　使用startRow和endRow限制


　　4Linux常用操作


　　aawk：


　　awk-F:`BEGIN{print"nameip"}{print$1$7}END{print"结束"}`/etc/passwd


　　last|head-5|awk`BEGIN{print"nameip"}{print$1$3}END{print"结束了"}`


　　bsed


　　5java线程2种方式实现、设计模式、链表操作、排序


　　(1)2种线程实现


　　aThread类继承


　　TestCLth=newTestCL()//类继承Thread


　　th.start()


　　b实现Runnable接口


　　Threadth=newThread(newRunnable(){


　　publicvoidrun(){


　　//实现


　　}


　　})


　　th.start()


　　(2)设计模式，分为4类


　　a创建模式：如工厂模式、单例模式


　　b结构模式：代理模式


　　c行为模式：观察者模式


　　d线程池模式


　　6【最熟悉的一个项目简介、架构图、使用的技术、你负责哪块】


　　7cdh集群监控


　　(1)数据库监控(2)主机监控(3)服务监控(4)活动监控


　　8计算机网络工作原理


　　将分散的机器通过数据通信原理连接起来，实现共享!


　　9hadoop生态系统


　　hdfsmapreducehivehbasezookeeperlume


　　hdfs原理及各个模块的功能mapreduce原理mapreduce优化数据倾斜


　　11系统维护：hadoop升级datanode节点


　　12【讲解项目要点：数据量、多少人、分工、运行时间、项目使用机器、算法、技术】


　　13【学会向对方提问】


　　14jvm运行机制及内存原理


　　运行：


　　I加载.class文件


　　II管理并且分配内存


　　III垃圾回收


　　内存原理：


　　IJVM装载环境和配置


　　II装载JVM.dll并初始化JVM.dll


　　IV处理class类


　　15hdfs、yarn参数调优


　　mapreduce.job.jvm.num.tasks


　　默认为1，设置为-1，重用jvm


　　16Hbase、Hive、impala、zookeeper、Storm、spark原理和使用方法、使用其架构图讲解


　　【某公司笔试题】


　　1、如何为一个hadoop任务设置mappers的数量


　　答案：


　　使用job.setNumMapTask(intn)手动分割，这是不靠谱的


　　官方文档：“Note:Thisisonlyahinttotheframework”说明这个方法只是提示作用，不起决定性作用


　　实际上要用公式计算：


　　Max(min.split，min(max.split，block))就设置分片的最大最下值computeSplitSize()设置


　　参考：深度分析如何在Hadoop中控制Map的数量 - 张贵宾的技术专栏 - 博客频道 - CSDN.NET


　　2、有可能使hadoop任务输出到多个目录中么?如果可以，怎么做?


　　答案：在1.X版本后使用MultipleOutputs.java类实现


　　源码：


　　MultipleOutputs.addNamedOutput(conf,"text2",TextOutputFormat.class,Long.class,String.class);


　　MultipleOutputs.addNamedOutput(conf,"text3",TextOutputFormat.class,Long.class,String.class);


　　参考：MapReduce中的自定义多目录/文件名输出HDFS - leejun2005的个人页面 - 开源中国社区


　　发音：Multiple['m?lt?pl]--》许多的


　　3、如何为一个hadoop任务设置要创建的reducer的数量


　　答案：job.setNumReduceTask(intn)


　　或者调整hdfs-site.xml中的mapred.tasktracker.reduce.tasks.maximum默认参数值


　　4、在hadoop中定义的主要公用InputFormats中，哪一个是默认值：


　　(A)TextInputFormat


　　(B)KeyValueInputFormat


　　(C)SequenceFileInputFormat


　　答案：A


　　5、两个类TextInputFormat和KeyValueTextInputFormat的区别?


　　答案：


　　?FileInputFormat的子类：


　　TextInputFormat(默认类型，键是LongWritable类型，值为Text类型，key为当前行在文件中的偏移量，value为当前行本身);


　　?KeyValueTextInputFormat(适合文件自带key，value的情况，只要指定分隔符即可，比较实用，默认是分割);


　　源码：


　　StringsepStr=job.get("mapreduce.input.keyvaluelinerecordreader.key.value.separator","");


　　注意：在自定义输入格式时，继承FileInputFormat父类


　　6、在一个运行的hadoop任务中，什么是InputSpilt?


　　答案：InputSplit是MapReduce对文件进行处理和运算的输入单位，只是一个逻辑概念，每个InputSplit并没有对文件实际的切割，只是记录了要处理的数据的位置(包括文件的path和hosts)和长度(由start和length决定)，默认情况下与block一样大。


　　拓展：需要在定义InputSplit后，展开讲解mapreduce的原理


　　7、Hadoop框架中，文件拆分是怎么被调用的?


　　答案：JobTracker，创建一个InputFormat的实例，调用它的getSplits()方法，把输入目录的文件拆分成FileSplist作为Mappertask的输入，生成Mappertask加入Queue。


　　源码中体现了拆分的数量


　　longgoalSize=totalSize/(numSplits==0?1:numSplits);


　　longminSize=Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.


　　FileInputFormat.SPLIT_MINSIZE,1),minSplitSize);//minSplitSize默认是1


　　8、分别举例什么情况下使用combiner,什么情况下不会使用?


　　答案：Combiner适用于对记录汇总的场景(如求和)，但是，求平均数的场景就不能使用Combiner了


　　9、Hadoop中job和Tasks之间的区别是什么?


　　答案：


　　job是工作的入口，负责控制、追踪、管理任务，也是一个进程


　　包含maptask和reducetask


　　Tasks是map和reduce里面的步骤，主要用于完成任务，也是线程


　　10、Hadoop中通过拆分任务到多个节点运行来实现并行计算，但是某些节点运行较慢会拖慢整个任务的运行，hadoop采用何种机制应对这种情况?


　　答案：结果查看监控日志，得知产生这种现象的原因是数据倾斜问题


　　解决：


　　(1)调整拆分mapper的数量(partition数量)


　　(2)增加jvm


　　(3)适当地将reduce的数量变大


　　11、流API中的什么特性带来可以使mapreduce任务可以以不同语言(如perlubyawk等)实现的灵活性?


　　答案：用可执行文件作为Mapper和Reducer，接受的都是标准输入，输出的都是标准输出


　　12、参考下面的M/R系统的场景：


　　--HDFS块大小为64MB


　　--输入类型为FileInputFormat


　　--有3个文件的大小分别是：64k65MB127MB


　　Hadoop框架会把这些文件拆分为多少块?


　　答案：


　　64k------->一个block


　　65MB---->两个文件：64MB是一个block，1MB是一个block


　　127MB--->两个文件：64MB是一个block,63MB是一个block


　　13、Hadoop中的RecordReader的作用是什么?


　　答案：属于split和mapper之间的一个过程


　　将inputsplit输出的行为一个转换记录，成为key-value的记录形式提供给mapper


　　14、Map阶段结束后，Hadoop框架会处理：Partitioning,shuffle和sort,在这个阶段都会发生了什么?


　　答案：


　　MR一共有四个阶段，splitmapshuffreduce在执行完map之后，可以对map的输出结果进行分区，


　　分区：这块分片确定到哪个reduce去计算(汇总)


　　排序：在每个分区中进行排序，默认是按照字典顺序。


　　Group：在排序之后进行分组


　　15、如果没有定义partitioner,那么数据在被送达reducer前是如何被分区的?


　　答案：


　　Partitioner是在map函数执行context.write()时被调用。


　　用户可以通过实现自定义的?Partitioner来控制哪个key被分配给哪个?Reducer。


　　查看源码知道：


　　如果没有定义partitioner，那么会走默认的分区Hashpartitioner


　　publicclassHashPartitionerextendsPartitioner{


　　/**Use{@linkObject#hashCode()}topartition.*/


　　publicintgetPartition(Kkey,Vvalue,intnumReduceTasks){


　　return(key.hashCode()&Integer.MAX_VALUE)%numReduceTasks;


　　}


　　}


　　16、什么是Combiner?


　　答案：这是一个hadoop优化性能的步骤，它发生在map与reduce之间


　　目的：解决了数据倾斜的问题，减轻网络压力，实际上时减少了maper的输出


　　源码信息如下：


　　publicvoidreduce(Textkey,Iteratorvalues,


　　OutputCollectoroutput,Reporterreporter)


　　throwsIOException{


　　LongWritablemaxValue=null;


　　while(values.hasNext()){


　　LongWritablevalue=values.next();


　　if(maxValue==null){


　　maxValue=value;


　　}elseif(value.compareTo(maxValue)>0){


　　maxValue=value;


　　}


　　}


　　output.collect(key,maxValue);


　　}


　　在collect实现类中，有这样一段方法


　　publicsynchronizedvoidcollect(Kkey,Vvalue)


　　throwsIOException{


　　outCounter.increment(1);


　　writer.append(key,value);


　　if((outCounter.getValue()%progressBar)==0){


　　progressable.progress();


　　}



　　}
　　
　　
　　



**1.0 简要描述如何安装配置apache的一个开源hadoop，只描述即可，无需列出具体步骤，列出具体步骤更好。**

答：第一题：1使用root账户登录

2 修改IP

3 修改host主机名

4 配置SSH免密码登录

5 关闭防火墙

6 安装JDK

6 解压hadoop安装包

7 配置hadoop的核心文件 hadoop-env.sh，core-site.xml , mapred-site.xml ， hdfs-site.xml

8 配置hadoop环境变量

9 格式化 hadoop namenode-format

10 启动节点start-all.sh

2.0 请列出正常的hadoop集群中hadoop都分别需要启动 哪些进程，他们的作用分别都是什么，请尽量列的详细一些。

答：namenode：负责管理hdfs中文件块的元数据，响应客户端请求，管理datanode上文件block的均衡，维持副本数量

Secondname:主要负责做checkpoint操作；也可以做冷备，对一定范围内数据做快照性备份。

Datanode:存储数据块，负责客户端对数据块的io请求

Jobtracker :管理任务，并将任务分配给 tasktracker。

Tasktracker: 执行JobTracker分配的任务。

Resourcemanager

Nodemanager

Journalnode

Zookeeper

Zkfc

3.0请写出以下的shell命令

（1）杀死一个job

（2）删除hdfs上的 /tmp/aaa目录

（3）加入一个新的存储节点和删除一个节点需要执行的命令

答：（1）hadoop job –list 得到job的id，然后执 行 hadoop job -kill jobId就可以杀死一个指定jobId的job工作了。

（2）hadoopfs -rmr /tmp/aaa

(3) 增加一个新的节点在新的几点上执行

Hadoop daemon.sh start datanode

Hadooop daemon.sh start tasktracker/nodemanager

下线时，要在conf目录下的excludes文件中列出要下线的datanode机器主机名

然后在主节点中执行 hadoop dfsadmin -refreshnodes à下线一个datanode

删除一个节点的时候，只需要在主节点执行

hadoop mradmin -refreshnodes ---à下线一个tasktracker/nodemanager

4.0 请列出你所知道的hadoop调度器，并简要说明其工作方法

答：Fifo schedular :默认，先进先出的原则

Capacity schedular :计算能力调度器，选择占用最小、优先级高的先执行，依此类推。

Fair schedular:公平调度，所有的 job 具有相同的资源。

5.0 请列出你在工作中使用过的开发mapreduce的语言

答：java，hive，（python，c++）hadoop streaming

6.0 当前日志采样格式为

a , b , c , d

b , b , f , e

a , a , c , f

请你用最熟悉的语言编写mapreduce，计算第四列每个元素出现的个数

答：

public classWordCount1 {

public static final String INPUT_PATH ="hdfs://hadoop0:9000/in";

public static final String OUT_PATH ="hdfs://hadoop0:9000/out";

public static void main(String[] args)throws Exception {

Configuration conf = newConfiguration();

FileSystem fileSystem =FileSystem.get(conf);

if(fileSystem.exists(newPath(OUT_PATH))){}

fileSystem.delete(newPath(OUT_PATH),true);

Job job = newJob(conf,WordCount1.class.getSimpleName());

//1.0读取文件，解析成key,value对

FileInputFormat.setInputPaths(job,newPath(INPUT_PATH));

//2.0写上自己的逻辑，对输入的可以，value进行处理，转换成新的key,value对进行输出

job.setMapperClass(MyMapper.class);

job.setMapOutputKeyClass(Text.class);

job.setMapOutputValueClass(LongWritable.class);

//3.0对输出后的数据进行分区

//4.0对分区后的数据进行排序，分组，相同key的value放到一个集合中

//5.0对分组后的数据进行规约

//6.0对通过网络将map输出的数据拷贝到reduce节点

//7.0 写上自己的reduce函数逻辑，对map输出的数据进行处理

job.setReducerClass(MyReducer.class);

job.setOutputKeyClass(Text.class);

job.setOutputValueClass(LongWritable.class);

FileOutputFormat.setOutputPath(job,new Path(OUT_PATH));

job.waitForCompletion(true);

}

static class MyMapper extendsMapper<LongWritable, Text, Text, LongWritable>{

[@Override](https://my.oschina.net/u/1162528)

protected void map(LongWritablek1, Text v1,

org.apache.hadoop.mapreduce.Mapper.Contextcontext)

throws IOException,InterruptedException {

String[] split =v1.toString().split("\t");

for(String words :split){

context.write(split[3],1);

}

}

}

static class MyReducer extends Reducer<Text,LongWritable, Text, LongWritable>{

protected void reduce(Text k2,Iterable<LongWritable> v2,

org.apache.hadoop.mapreduce.Reducer.Contextcontext)

throws IOException,InterruptedException {

Long count = 0L;

for(LongWritable time :v2){

count += time.get();

}

context.write(v2, newLongWritable(count));

}

}

}

7.0 你认为用java ， streaming ， pipe方式开发map/reduce ， 各有哪些优点

就用过 java 和 hiveQL。

Java 写 mapreduce 可以实现复杂的逻辑，如果需求简单，则显得繁琐。

HiveQL 基本都是针对 hive 中的表数据进行编写，但对复杂的逻辑（杂）很难进行实现。写起来简单。

8.0 hive有哪些方式保存元数据，各有哪些优点

三种：自带内嵌数据库derby，挺小，不常用，只能用于单节点

mysql常用

上网上找了下专业名称：single user mode..multiuser mode...remote user mode

9.0 请简述hadoop怎样实现二级排序（就是对key和value双排序）

第一种方法是，Reducer将给定key的所有值都缓存起来，然后对它们再做一个Reducer内排序。但是，由于Reducer需要保存给定key的所有值，可能会导致出现内存耗尽的错误。

第二种方法是，将值的一部分或整个值加入原始key，生成一个组合key。这两种方法各有优势，第一种方法编写简单，但并发度小，数据量大的情况下速度慢(有内存耗尽的危险)，

第二种方法则是将排序的任务交给MapReduce框架shuffle，更符合Hadoop/Reduce的设计思想。这篇文章里选择的是第二种。我们将编写一个Partitioner，确保拥有相同key(原始key，不包括添加的部分)的所有数据被发往同一个Reducer，还将编写一个Comparator，以便数据到达Reducer后即按原始key分组。

10.简述hadoop实现jion的几种方法

Map side join----大小表join的场景，可以借助distributed cache

Reduce side join

11.0 请用java实现非递归二分查询

\1. public class BinarySearchClass

\2. {

3.

\4. public static int binary_search(int[] array, int value)

\5. {

\6. int beginIndex = 0;// 低位下标

\7. int endIndex = array.length - 1;// 高位下标

\8. int midIndex = -1;

\9. while (beginIndex **<**= endIndex) {

10.

midIndex = beginIndex + (endIndex - beginIndex) / 2;//防止溢出

\11. if (value == array[midIndex]) {

12.

return midIndex;

\13. } else if (value **< array**[midIndex]) {

14.

endIndex = midIndex - 1;

\15. } else {

16.

beginIndex = midIndex + 1;

\17. }

18.

}

\19. return -1;

20.

//找到了，返回找到的数值的下标，没找到，返回-1

\21. }

22.

23.

24.

//start 提示：自动阅卷起始唯一标识，请勿删除或增加。

\25. public static void main(String[] args)

26.

{

\27. System.out.println("Start...");

28.

int[] myArray = new int[] { 1, 2, 3, 5, 6, 7, 8, 9 };

\29. System.out.println("查找数字8的下标：");

30.

System.out.println(binary_search(myArray, 8));

\31. }

32.

//end //提示：自动阅卷结束唯一标识，请勿删除或增加。

\33. }

12.0 请简述mapreduce中的combine和partition的作用

答：combiner是发生在map的最后一个阶段，其原理也是一个小型的reducer，主要作用是减少输出到reduce的数据量，缓解网络传输瓶颈，提高reducer的执行效率。

partition的主要作用将map阶段产生的所有kv对分配给不同的reducer task处理，可以将reduce阶段的处理负载进行分摊

13.0 hive内部表和外部表的区别

Hive 向内部表导入数据时，会将数据移动到数据仓库指向的路径；若是外部表，数据的具体存放目录由用户建表时指定

在删除表的时候，内部表的元数据和数据会被一起删除，

而外部表只删除元数据，不删除数据。

这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。

\14. Hbase的rowKey怎么创建比较好？列簇怎么创建比较好？

答：

rowKey最好要创建有规则的rowKey，即最好是有序的。

经常需要批量读取的数据应该让他们的rowkey连续；

将经常需要作为条件查询的关键词组织到rowkey中；

列族的创建：

按照业务特点，把数据归类，不同类别的放在不同列族

\15. 用mapreduce怎么处理数据倾斜问题

本质：让各分区的数据分布均匀

可以根据业务特点，设置合适的partition策略

如果事先根本不知道数据的分布规律，利用随机抽样器抽样后生成partition策略再处理

\16. hadoop框架怎么来优化

答：

可以从很多方面来进行：比如hdfs怎么优化，mapreduce程序怎么优化，yarn的job调度怎么优化，hbase优化，hive优化。。。。。。。

\17. hbase内部机制是什么

答：

Hbase是一个能适应联机业务的数据库系统

物理存储：hbase的持久化数据是存放在hdfs上

存储管理：一个表是划分为很多region的，这些region分布式地存放在很多regionserver上

Region内部还可以划分为store，store内部有memstore和storefile

版本管理：hbase中的数据更新本质上是不断追加新的版本，通过compact操作来做版本间的文件合并

Region的split

集群管理：zookeeper + hmaster（职责） + hregionserver（职责）

\18. 我们在开发分布式计算job的时候，是否可以去掉reduce阶段

答：可以，例如我们的集群就是为了存储文件而设计的，不涉及到数据的计算，就可以将mapReduce都省掉。

比如，流量运营项目中的行为轨迹增强功能部分

怎么样才能实现去掉reduce阶段

去掉之后就不排序了，不进行shuffle操作了

19 hadoop中常用的数据压缩算法

答：

Lzo

Gzip

Default

Snapyy

如果要对数据进行压缩，最好是将原始数据转为SequenceFile 或者 Parquet File（spark）

\20. mapreduce的调度模式（题意模糊，可以理解为yarn的调度模式，也可以理解为mr的内部工作流程）

答： appmaster作为调度主管，管理maptask和reducetask

Appmaster负责启动、监控maptask和reducetask

Maptask处理完成之后，appmaster会监控到，然后将其输出结果通知给reducetask，然后reducetask从map端拉取文件，然后处理；

当reduce阶段全部完成之后，appmaster还要向resourcemanager注销自己

\21. hive底层与数据库交互原理

答：

Hive的查询功能是由hdfs + mapreduce结合起来实现的

Hive与mysql的关系：只是借用mysql来存储hive中的表的元数据信息，称为metastore

\22. hbase过滤器实现原则

答：可以说一下过滤器的父类（比较过滤器，专用过滤器）

过滤器有什么用途：

增强hbase查询数据的功能

减少服务端返回给客户端的数据量

\23. reduce之后数据的输出量有多大（结合具体场景，比如pi）

Sca阶段的增强日志（1.5T---2T）

过滤性质的mr程序，输出比输入少

解析性质的mr程序，输出比输入多（找共同朋友）

\24. 现场出问题测试mapreduce掌握情况和hive的ql语言掌握情况

25.datanode在什么情况下不会备份数据

答：在客户端上传文件时指定文件副本数量为1

26.combine出现在哪个过程

答：shuffle过程中

具体来说，是在maptask输出的数据从内存溢出到磁盘，可能会调多次

Combiner使用时候要特别谨慎，不能影响最后的逻辑结果

\27. hdfs的体系结构

答：

集群架构：

namenode datanode secondarynamenode

(active namenode ,standby namenode)journalnode zkfc

内部工作机制：

数据是分布式存储的

对外提供一个统一的目录结构

对外提供一个具体的响应者（namenode）

数据的block机制，副本机制

Namenode和datanode的工作职责和机制

读写数据流程

\28. flush的过程

答：flush是在内存的基础上进行的，首先写入文件的时候，会先将文件写到内存中，当内存写满的时候，一次性的将文件全部都写到硬盘中去保存，并清空缓存中的文件，

\29. 什么是队列

答：是一种调度策略，机制是先进先出

\30. List与set的区别

答：List和Set都是接口。他们各自有自己的实现类，有无顺序的实现类，也有有顺序的实现类。

最大的不同就是List是可以重复的。而Set是不能重复的。

List适合经常追加数据，插入，删除数据。但随即取数效率比较低。

Set适合经常地随即储存，插入，删除。但是在遍历时效率比较低。

31.数据的三范式

答：

第一范式（）无重复的列

第二范式（2NF）属性完全依赖于主键 [消除部分子函数依赖]

第三范式（3NF）属性不依赖于其它非主属性 [消除传递依赖]

32.三个datanode中当有一个datanode出现错误时会怎样？

答：

Namenode会通过心跳机制感知到datanode下线

会将这个datanode上的block块在集群中重新复制一份，恢复文件的副本数量

会引发运维团队快速响应，派出同事对下线datanode进行检测和修复，然后重新上线

33.sqoop在导入数据到mysql中，如何不重复导入数据，如果存在数据问题，sqoop如何处理？

答：FAILED java.util.NoSuchElementException

此错误的原因为sqoop解析文件的字段与MySql数据库的表的字段对应不上造成的。因此需要在执行的时候给sqoop增加参数，告诉sqoop文件的分隔符，使它能够正确的解析文件字段。

hive默认的字段分隔符为'\001'

34.描述一下hadoop中，有哪些地方使用到了缓存机制，作用分别是什么？

答：

Shuffle中

Hbase----客户端/regionserver

35.MapReduce优化经验

答：(1.)设置合理的map和reduce的个数。合理设置blocksize

(2.)避免出现数据倾斜

(3.combine函数

(4.对数据进行压缩

(5.小文件处理优化：事先合并成大文件，combineTextInputformat，在hdfs上用mapreduce将小文件合并成SequenceFile大文件（key:文件名，value：文件内容）

(6.参数优化

36.请列举出曾经修改过的/etc/下面的文件，并说明修改要解决什么问题？

答：/etc/profile这个文件，主要是用来配置环境变量。让hadoop命令可以在任意目录下面执行。

/ect/sudoers

/etc/hosts

/etc/sysconfig/network

/etc/inittab

37.请描述一下开发过程中如何对上面的程序进行性能分析，对性能分析进行优化的过程。

\38. 现有 1 亿个整数均匀分布，如果要得到前 1K 个最大的数，求最优的算法。

参见《海量数据算法面试大全》

39.mapreduce的大致流程

答：主要分为八个步骤

1/对文件进行切片规划

2/启动相应数量的maptask进程

3/调用FileInputFormat中的RecordReader，读一行数据并封装为k1v1

4/调用自定义的map函数，并将k1v1传给map

5/收集map的输出，进行分区和排序

6/reduce task任务启动，并从map端拉取数据

7/reduce task调用自定义的reduce函数进行处理

8/调用outputformat的recordwriter将结果数据输出

41.用mapreduce实现sql语 select count (x) from a group by b;

44.搭建hadoop集群 ， master和slaves都运行哪些服务

答：master主要是运行我们的主节点，slaves主要是运行我们的从节点。

\45. hadoop参数调优

\46. pig , latin , hive语法有什么不同

答：

\46. 描述Hbase，ZooKeeper搭建过程

48.hadoop运行原理

答：hadoop的主要核心是由两部分组成，HDFS和mapreduce，首先HDFS的原理就是分布式的文件存储系统，将一个大的文件，分割成多个小的文件，进行存储在多台服务器上。

Mapreduce的原理就是使用JobTracker和TaskTracker来进行作业的执行。Map就是将任务展开，reduce是汇总处理后的结果。

49.mapreduce的原理

答：mapreduce的原理就是将一个MapReduce框架由一个单独的master JobTracker和每个集群节点一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由maste指派的任务。

50.HDFS存储机制

答：HDFS主要是一个分布式的文件存储系统，由namenode来接收用户的操作请求，然后根据文件大小，以及定义的block块的大小，将大的文件切分成多个block块来进行保存

51.举一个例子说明mapreduce是怎么运行的。

Wordcount

52.如何确认hadoop集群的健康状况

答：有完善的集群监控体系（ganglia，nagios）

Hdfs dfsadmin –report

Hdfs haadmin –getServiceState nn1

53.mapreduce作业，不让reduce输出，用什么代替reduce的功能。

54.hive如何调优

答：hive最终都会转化为mapreduce的job来运行，要想hive调优，实际上就是mapreduce调优，可以有下面几个方面的调优。解决收据倾斜问题，减少job数量，设置合理的map和reduce个数，对小文件进行合并，优化时把握整体，单个task最优不如整体最优。按照一定规则分区。

55.hive如何控制权限

我们公司没做，不需要

56.HBase写数据的原理是什么？

答：

57.hive能像关系型数据库那样建多个库吗？

答：当然能了。

58.HBase宕机如何处理

答：宕机分为HMaster宕机和HRegisoner宕机，如果是HRegisoner宕机，HMaster会将其所管理的region重新分布到其他活动的RegionServer上，由于数据和日志都持久在HDFS中，该操作不会导致数据丢失。所以数据的一致性和安全性是有保障的。

如果是HMaster宕机，HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。即ZooKeeper会保证总会有一个HMaster在对外提供服务。

59.假设公司要建一个数据中心，你会如何处理？

先进行需求调查分析

设计功能划分

架构设计

吞吐量的估算

采用的技术类型

软硬件选型

成本效益的分析

项目管理

扩展性

安全性，稳定性

**60. 单项选择题**

\1. 下面哪个程序负责 HDFS 数据存储。 答案 C

a)NameNode b)Jobtracker c)Datanoded)secondaryNameNode e)tasktracker

\2. HDfS 中的 block 默认保存几份？ 答案 A

a)3 份 b)2 份 c)1 份 d)不确定

\3. 下列哪个程序通常与 NameNode 在一个节点启动？

a)SecondaryNameNode b)DataNodec)TaskTracker d)Jobtracker e)zkfc

\4. Hadoop 作者 答案D

a)Martin Fowler b)Kent Beck c)Doug cutting

\5. HDFS 默认 Block Size 答案 B

a)32MB b)64MB c)128MB

\6. 下列哪项通常是集群的最主要瓶颈 答案d

a)CPU b)网络 c)磁盘 d)内存

\7. 关于 SecondaryNameNode 哪项是正确的？ 答案C

a)它是NameNode的热备

b)它对内存没有要求

c)它的目的是帮助 NameNode 合并编辑日志，减少 NameNode 启动时间

d)SecondaryNameNode 应与 NameNode 部署到一个节点

**多选题：**

\8. 下列哪项可以作为集群的管理工具 答案 ABCD (此题出题有误)

a)Puppet b)Pdsh c)Cloudera Manager d)Zookeeper

\9. 配置机架感知的下面哪项正确

答案 ABC

a)如果一个机架出问题，不会影响数据读写

b)写入数据的时候会写到不同机架的 DataNode 中

c)MapReduce 会根据机架获取离自己比较近的网络数据

\10. Client 端上传文件的时候下列哪项正确 答案BC

a)数据经过 NameNode 传递给 DataNode

b)Client 端将文件切分为 Block，依次上传

c)Client 只上传数据到一台 DataNode，然后由 NameNode 负责 Block 复制工作

\11. 下列哪个是 Hadoop 运行的模式 答案 ABC

a)单机版 b)伪分布式 c)分布式

\12. Cloudera 提供哪几种安装 CDH 的方法 答案 ABCD

a)Cloudera manager b)Tar ball c)Yum d)Rpm

**判断题：**全部都是错误滴

\13. Ganglia 不仅可以进行监控，也可以进行告警。（ ）

\14. Block Size 是不可以修改的。（ ）

\15. Nagios 不可以监控 Hadoop 集群，因为它不提供 Hadoop 支持。（ ）

\16. 如果 NameNode 意外终止， SecondaryNameNode 会接替它使集群继续工作。（ ）

\17. Cloudera CDH 是需要付费使用的。（ ）

\18. Hadoop 是 Java 开发的，所以 MapReduce 只支持 Java 语言编写。（ ）

\19. Hadoop 支持数据的随机读写。（ ）

\20. NameNode 负责管理 metadata， client 端每次读写请求，它都会从磁盘中读取或则

会写入 metadata 信息并反馈 client 端。（ ）

\21. NameNode 本地磁盘保存了 Block 的位置信息。（ ）

\22. DataNode 通过长连接与 NameNode 保持通信。（ ）

\23. Hadoop 自身具有严格的权限管理和安全措施保障集群正常运行。（ ）

\24. Slave节点要存储数据，所以它的磁盘越大越好。（ ）

\25. hadoop dfsadmin –report 命令用于检测 HDFS 损坏块。（ ）

\26. Hadoop 默认调度器策略为 FIFO（ ）

\27. 集群内每个节点都应该配 RAID，这样避免单磁盘损坏，影响整个节点运行。（ ）

\28. 因为 HDFS 有多个副本，所以 NameNode 是不存在单点问题的。（ ）

\29. 每个 map 槽（进程）就是一个线程。（ ）

\30. Mapreduce 的 input split 就是一个 block。（ ）

\31. NameNode的默认Web UI 端口是 50030，它通过 jetty 启动的 Web 服务。（ ）

\32. Hadoop 环境变量中的 HADOOP_HEAPSIZE 用于设置所有 Hadoop 守护线程的内存。它默认是200 GB。（ ）

\33. DataNode 首次加入 cluster 的时候，如果 log中报告不兼容文件版本，那需要

NameNode执行“Hadoop namenode -format”操作格式化磁盘。（ ）

\63. 谈谈 hadoop1 和 hadoop2 的区别

答：

hadoop1的主要结构是由HDFS和mapreduce组成的，HDFS主要是用来存储数据，mapreduce主要是用来计算的，那么HDFS的数据是由namenode来存储元数据信息，datanode来存储数据的。Jobtracker接收用户的操作请求之后去分配资源执行task任务。

在hadoop2中，首先避免了namenode单点故障的问题，使用两个namenode来组成namenode feduration的机构，两个namenode使用相同的命名空间，一个是standby状态，一个是active状态。用户访问的时候，访问standby状态，并且，使用journalnode来存储数据的原信息，一个namenode负责读取journalnode中的数据，一个namenode负责写入journalnode中的数据，这个平台组成了hadoop的HA就是high availableAbility高可靠。

然后在hadoop2中没有了jobtracker的概念了，统一的使用yarn平台来管理和调度资源，yarn平台是由resourceManager和NodeManager来共同组成的，ResourceManager来接收用户的操作请求之后，去NodeManager上面启动一个主线程负责资源分配的工作，然后分配好了资源之后告知ResourceManager，然后ResourceManager去对应的机器上面执行task任务。

\64. 说说值对象与引用对象的区别？

\65. 谈谈你对反射机制的理解及其用途？

答：java中的反射，首先我们写好的类，经过编译之后就编程了.class文件，我们可以获取这个类的.class文件，获取之后，再来操作这个类。这个就是java的反射机制。

\66. ArrayList、Vector、LinkedList 的区别及其优缺点？HashMap、HashTable 的区别及其优缺点？

答：ArrayList 和Vector是采用数组方式存储数据， ，Vector由于使用了synchronized方法（线程安全）所以性能上比ArrayList要差，LinkedList使用双向链表实现存储，按序号索引数据需要进行向前或向后遍历，但是插入数据时只需要记录本项的前后项即可，所以插入数度较快！

HashMap和HashTable：Hashtable的方法是同步的，而HashMap的方法不是，Hashtable是基于陈旧的Dictionary类的，HashMap是Java 1.2引进的Map接口的一个实现。HashMap是一个线程不同步的，那么就意味着执行效率高，HashTable是一个线程同步的就意味着执行效率低，但是HashMap也可以将线程进行同步，这就意味着，我们以后再使用中，尽量使用HashMap这个类。

\67. 文件大小默认为 64M，改为 128M 有啥影响？

答：更改文件的block块大小，需要根据我们的实际生产中来更改block的大小，如果block定义的太小，大的文件都会被切分成太多的小文件，减慢用户上传效率，如果block定义的太大，那么太多的小文件可能都会存到一个block块中，虽然不浪费硬盘资源，可是还是会增加namenode的管理内存压力。

\69. RPC 原理？

答：

1.调用客户端句柄；执行传送参数

2.调用本地系统内核发送网络消息

\3. 消息传送到远程主机

\4. 服务器句柄得到消息并取得参数

\5. 执行远程过程

\6. 执行的过程将结果返回服务器句柄

\7. 服务器句柄返回结果，调用远程系统内核

\8. 消息传回本地主机

\9. 客户句柄由内核接收消息

\10. 客户接收句柄返回的数据

\70. 对 Hadoop 有没有调优经验，没有什么使用心得？（调优从参数调优讲起）

dfs.block.size

Mapredure：

io.sort.mb

io.sort.spill.percent

mapred.local.dir

mapred.map.tasks &mapred.tasktracker.map.tasks.maximum

mapred.reduce.tasks &mapred.tasktracker.reduce.tasks.maximum

mapred.reduce.max.attempts

mapred.reduce.parallel.copies

mapreduce.reduce.shuffle.maxfetchfailures

mapred.child.java.opts

mapred.reduce.tasks.speculative.execution

mapred.compress.map.output &mapred.map.output.compression.codec

mapred.reduce.slowstart.completed.maps

72以你的实际经验，说下怎样预防全表扫描

答：

1.应尽量避免在where 子句中对字段进行null 值判断，否则将导致引擎放弃使用索引而进行全表扫描

2.应尽量避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫

3.描应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行

全表扫描

4.in 和 not in，用具体的字段列表代替，不要返回用不到的任何字段。in 也要慎用，否则会导致全表扫描

5.避免使用模糊查询

6.任何地方都不要使用select* from t

\73. zookeeper 优点，用在什么场合

答：极大方便分布式应用的开发；（轻量，成本低，性能好，稳定性和可靠性高）

75.把公钥追加到授权文件的命令？该命令是否在 root 用户下执行？

答：ssh-copy-id

哪个用户需要做免密登陆就在哪个用户身份下执行

\76. HadoopHA 集群中各个服务的启动和关闭的顺序？

答：

\77. 在 hadoop 开发过程中使用过哪些算法？其应用场景是什么？

答：排序，分组，topk，join，group

\78. 在实际工作中使用过哪些集群的运维工具，请分别阐述期作用。

答：nmon ganglia nagios

\79. 一台机器如何应对那么多的请求访问，高并发到底怎么实现，一个请求怎么产生的，

在服务端怎么处理的，最后怎么返回给用户的，整个的环节操作系统是怎么控制的？

\80. java 是传值还是传址？

答：引用传递。传址

\81. 问：你们的服务器有多少台？

100多台

\82. 问：你们服务器的内存多大？

128G或者64G的

\83. hbase 怎么预分区？

建表时可以通过shell命令预分区，也可以在代码中建表做预分区

《具体命令详见笔记汇总》

\84. hbase 怎么给 web 前台提供接口来访问（HTABLE可以提供对 HBase的访问，但是怎么查询同一条记录的多个版本数据）？

答：使用HTable来提供对HBase的访问，可以使用时间戳来记录一条数据的多个版本。

\85. .htable API 有没有线程安全问题，在程序中是单例还是多例？

多例：当多线程去访问同一个表的时候会有。

\86. 你们的数据是用什么导入到数据库的？导入到什么数据库？

处理完成之后的导出：利用hive 处理完成之后的数据，通过sqoop 导出到 mysql 数据库

中，以供报表层使用。

\87. 你们业务数据量多大？有多少行数据？(面试了三家，都问这个问题)

开发时使用的是部分数据，不是全量数据，有将近一亿行（8、9 千万，具体不详，一般开

发中也没人会特别关心这个问题）

\88. 你们处理数据是直接读数据库的数据还是读文本数据？

将日志数据导入到 hdfs 之后进行处理

\89. 你们写 hive 的 hql 语句，大概有多少条？

不清楚，我自己写的时候也没有做过统计

\90. 你们提交的 job 任务大概有多少个？这些job 执行完大概用多少时间？(面试了三家，都问这个问题)

没统计过，加上测试的，会有很多

Sca阶段，一小时运行一个job，处理时间约12分钟

Etl阶段，有2千多个job，从凌晨12:00开始次第执行，到早上5点左右全部跑完

\91. hive 跟 hbase 的区别是？

答：Hive和Hbase是两种基于Hadoop的不同技术--Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。当然，这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到Hbase，设置再从Hbase写回Hive。

\92. 你在项目中主要的工作任务是？

Leader

预处理系统、手机位置实时查询系统，详单系统，sca行为轨迹增强子系统，内容识别中的模板匹配抽取系统

设计、架构、技术选型、质量把控，进度节点把握。。。。。。

\93. 你在项目中遇到了哪些难题，是怎么解决的？

Storm获取实时位置信息动态端口的需求

\101. job 的运行流程(提交一个 job 的流程)？

102Hadoop 生态圈中各种框架的运用场景？

\103. hive 中的压缩格式 RCFile、TextFile、SequenceFile

[M5] 各有什么区别？

以上 3 种格式一样大的文件哪个占用空间大小..等等

采用RCfile的格式读取的数据量（373.94MB）远远小于sequenceFile的读取量（2.59GB）

2、执行速度前者(68秒)比后者(194秒)快很多

从以上的运行进度看，snappy的执行进度远远高于bz的执行进度。

在hive中使用压缩需要灵活的方式，如果是数据源的话，采用RCFile+bz或RCFile+gz的方式，这样可以很大程度上节省磁盘空间；而在计算的过程中，为了不影响执行的速度，可以浪费一点磁盘空间，建议采用RCFile+snappy的方式，这样可以整体提升hive的执行速度。

至于lzo的方式，也可以在计算过程中使用，只不过综合考虑（速度和压缩比）还是考虑snappy适宜。

104假如：Flume 收集到的数据很多个小文件,我需要写 MR 处理时将这些文件合并

(是在 MR 中进行优化,不让一个小文件一个 MapReduce)

他们公司主要做的是中国电信的流量计费为主,专门写 MR。

\105. 解释“hadoop”和“hadoop 生态系统”两个概念

\109. MapReduce 2.0”与“YARN”是否等同，尝试解释说明

MapReduce 2.0 --àmapreduce + yarn

\110. MapReduce 2.0 中，MRAppMaster 主要作用是什么，MRAppMaster 如何实现任务

容错的？

\111. 为什么会产生 yarn,它解决了什么问题，有什么优势？

\114. 数据备份,你们是多少份,如果数据超过存储容量,你们怎么处理？

3份，多加几个节点

\115. 怎么提升多个 JOB 同时执行带来的压力,如何优化,说说思路？

增加运算能力

\116. 你们用 HBASE 存储什么数据？

流量详单

\117. 你们的 hive 处理数据能达到的指标是多少？

118.hadoop中RecorderReader的作用是什么？？？

1、 在hadoop中定义的主要公用InputFormat中，哪个是默认值？ FileInputFormat

2、 两个类TextInputFormat和KeyValueInputFormat的区别是什么？

答：TextInputFormat主要是用来格式化输入的文本文件的，KeyValueInputFormat则主要是用来指定输入输出的key,value类型的

3、 在一个运行的hadoop任务中，什么是InputSplit？

InputSplit是InputFormat中的一个方法，主要是用来切割输入文件的，将输入文件切分成多个小文件，

然后每个小文件对应一个map任务

4、 Hadoop框架中文件拆分是怎么调用的？

InputFormat --> TextInputFormat -->RecordReader --> LineRecordReader --> LineReader

5、 参考下列M/R系统的场景：hdfs块大小为64MB，输入类为FileInputFormat，有3个文件的大小分别为64KB, 65MB, 127MB

会产生多少个maptask 4个 65M这个文件只有一个切片《原因参见笔记汇总TextInputformat源码分析部分》

8、 如果没有自定义partitioner，那数据在被送达reducer前是如何被分区的？

hadoop有一个默认的分区类，HashPartioer类，通过对输入的k2去hash值来确认map输出的k2,v2送到哪一个reduce中去执行。

10、分别举例什么情况要使用 combiner，什么情况不使用？

求平均数的时候就不需要用combiner，因为不会减少reduce执行数量。在其他的时候，可以依据情况，使用combiner，来减少map的输出数量，减少拷贝到reduce的文件，从而减轻reduce的压力，节省网络开销，提升执行效率

11、Hadoop中job和tasks之间的区别是什么？

Job是我们对一个完整的mapreduce程序的抽象封装

Task是job运行时，每一个处理阶段的具体实例，如map task，reduce task，maptask和reduce task都会有多个并发运行的实例

12、hadoop中通过拆分任务到多个节点运行来实现并行计算，但某些节点运行较慢会拖慢整个任务的运行，hadoop采用全程机制应对这个情况？

Speculate 推测执行

14、有可能使hadoop任务输出到多个目录中吗？如果可以，怎么做？

自定义outputformat或者用multioutputs工具

15、如何为一个hadoop任务设置mappers的数量？

Split机制

16、如何为一个hadoop任务设置要创建reduder的数量？

可以通过代码设置

具体设置多少个，应该根据硬件配置和业务处理的类型来决定

下面是HBASE我非常不懂的地方：

1.hbase怎么预分区？

2.hbase怎么给web前台提供接口来访问（HTABLE可以提供对HTABLE的访问，但是怎么查询同一条记录的多个版本数据）？

3.htable API有没有线程安全问题，在程序中是单例还是多例？

4.我们的hbase大概在公司业务中（主要是网上商城）大概4个表，几个表簇，大概都存什么样的数据？

下面的Storm的问题：

1.metaq消息队列 zookeeper集群 storm集群（包括zeromq,jzmq,和storm本身）就可以完成对商城推荐系统功能吗？还有没有其他的中间件？





**阿里巴巴**

阿里巴巴的面试轮次基本是三次，技术方面分为简单的项目经验了解和个人技术水平考察。部分网友将面试官分成了两类：一类是技术型，不断加深问题难度以测量你对技术的掌握深度;另一类是综合素质型，通过简单的技术问题引导求职者自我表达，侧重考察求职者的个人思想、表达能力等综合素质。

二叉树、排序、SQL等基本概念问题几乎所有求职者都被问到过，接下来的手写代码或者技术深度类的问题都与面试的职位息息相关。有技术人员贴出了自己对阿里巴巴大数据类面试题的一些总结：

如果参加过阿里巴巴的大数据竞赛或许对面试有很大帮助。面试中，可以抓住面试官的兴趣点，尽量拉到你擅长的话题或技术讨论中，这样对你的面试有很大帮助。

![大数据工程师面试题集锦：互联网公司篇！](http://s4.51cto.com/oss/201804/16/14ba4e8939895e94a08e8884f19b3417.png-wh_600x-s_256701756.png)

![大数据工程师面试题集锦：互联网公司篇！](http://s1.51cto.com/oss/201804/16/f0684111f7b8b14aca9ee6835067f162.png-wh_600x-s_618376222.png)

![大数据工程师面试题集锦：互联网公司篇！](http://s3.51cto.com/oss/201804/16/bd9a00a19da203f5825dc34f2f35a41d.png-wh_600x-s_1928993509.png)

**京东**

京东的大数据类面试基本是两轮，再加一轮人力面。人力是简单了解一些工作情况、期望薪资之类的问题。前两轮面试一轮是基础面，一轮是项目面。第一轮主要是围绕基础概念展开，基础知识扎实肯定没问题。第二轮会根据项目经验进行询问，技术难度加大。

问题主要围绕Hadoop、Spark、Hive、Storm和数据库的基础操作以及工作原理，Linux的常用指令等，详细询问你的项目经验，了解项目中用到的技术或方法，遇到的问题，必要时需要手写代码或一些大数据组件的搭建和详细参数配置。

**腾讯**

腾讯需要经过三轮面试，基本流程与京东相似。简历上的项目经验一定会被仔细询问，建议求职者对自己所写的项目经验认真评估，对项目的每一个细节都仔细查证。其次，部分技术人员表示自己被要求手写代码，比如用C++写深度优先搜索;与Java有关的面试题也不少，比如Java有几种设计模式，各有什么优缺点。适合哪些场景;最后是一些与Hadoop相关的问题，比如Hadoop的一些操作如何实现等。

**百度**

一共三轮面试，技术方面主要考察各类排序算法、Linux命令、Hadoop、分布式、大数据处理方面的内容，对统计模型要求较高，对数据结构以及编程功底有要求。多位求职者反应，百度的面试更偏重对技术水平的考察，偏向一题多解，不但要给出方案，同时要不断优化得到最优解决方案。

![大数据工程师面试题集锦：互联网公司篇！](http://s5.51cto.com/oss/201804/16/affc10cf99b8f01b77d894a761dbc18d.png)

**美团**

美团的面试轮数同样在3到4轮之间，不同的面试官侧重点可能有所不同，比如侧重关系型数据库的可能会问你索引为何选B树。B+树，不选二叉树，事务的ACID是什么，传统关系型数据库和非关系型数据库HBase的区别，HBase删除数据怎么做(时间戳切入)等;侧重大数据，尤其是离线处理方向的可能会问你离线数据处理怎么做，基于磁盘的离线处理和基于内存处理海量数据的区别，MapReduce和Hive的区别，HBase和HDFS的区别等。

![大数据工程师面试题集锦：互联网公司篇！](http://s3.51cto.com/oss/201804/16/b8f7b25744795bc304386bdebaafba81.png)

各种排序算法几乎是各家公司面试都会被问到的问题，快速排序出现的频率尤其之高。

**滴滴**

滴滴的大数据类面试题首先是常规的自我介绍，应聘互联网公司，自我介绍中可以适当突出自己的合作能力和对加班的看法(互联网公司加班现象比较普遍，这点需要做好心理准备)然后是简单的项目经验询问，涉及的原理、算法、公式推导、算法调优等。由于业务特性，滴滴对服务端高并发的问题比较看重，例如，有技术人员被问到在打车高峰期流量较高的情况下，如何让客户省钱。其他问题包括Hadoop提交作业的总体流程、HDFS的基本原理、Hadoop的shuffle过程、SQL语句自动规范化处理等。算法题目还是排序、递归、遍历等常见题目。

**今日头条**

今日头条的面试安排比较人性化，基本会和求职者商量时间，如果时间不合适，也会考虑视频面试，比较灵活。面试问题因人而异，如果项目经历基本空白，面试官会着重考察基础概念的掌握，面试时间半小时到一小时不等。

![大数据工程师面试题集锦：互联网公司篇！](http://s5.51cto.com/oss/201804/16/380d6220a3607777be79c250356a8c4b.png)

基础概念部分首先考察对编程语言的理解，Java或者C++/C任选其一，比如Java方面，会询问你虚拟机、类加载机制、垃圾回收机制等问题;其二是对网络基础的考察，三次握手四次挥手(几乎每家公司面试都会提到)、Http协议等;其三是对大数据框架的理解，比如常见的MapReduce理解、shuffle过程理解等;最后是对基础算法的现场编程，比如一个数1234，得到下一个比它大的即1243以此类推。

对于项目经验比较丰富的技术人员，面试时对项目经验的盘问会多一些，代码题的难度会高一些，但据前线面试人员反馈，今日头条在代码和算法考察方面，都是一些基础算法，难度不大，但就怕一时之间技术人员反应不出最优解，很多问题都有多种方法可以解答，如果知道可以都写出来。

**小米、华为**

小米和华为的面试过程比较简洁，更多的是业务能力和项目经验的考察。面试题与其他几家公司的大致雷同，面试过程相应较短，同样需要手写代码。

**总结**

1. 项目经验是所有互联网公司面试时都会认真询问的，因此简单参与或相关度极低的项目不建议写在简历上。同时，对项目中用到的相关算法、遇到的技术难点一定要心里有数。
2. 算法能力依然重要，排序算法、深度、广度遍历搜索是出现频率最高的算法。数据结构方面，堆、哈希表、二叉树是最重要被考到的概念。网络基础部分，三次握手四次挥手和Http协议几乎必问。
3. 大数据框架部分，不同公司的业务不同关注点不同。但是基本的框架或生态组件的理解，比如MapReduce、Hive和HDFS是被问频率最高的问题，也包括一些组件之间的对比。
4. 综合素质方面，合作能力是重要考察方面，可以在项目经验中适当增加对合作方面的说明。其次，自己对加班的态度也很重要，互联网公司的加班是常态，求职时需要做好心理准备。



大纲

kafka的message包括哪些信息



怎么查看kafka的offset



一、Map端的shuffle



二、Reduce端的shuffle



spark集群运算的模式



读：

1、跟namenode通信查询元数据，找到文件块所在的datanode服务器

2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流

3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）

4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件

写：

1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在

2、namenode返回是否可以上传

3、client请求第一个 block该传输到哪些datanode服务器上

4、namenode返回3个datanode服务器ABC

5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端

6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答

7、当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。

RDD中reduceBykey与groupByKey哪个性能好，为什么

reduceByKey：reduceByKey会在结果发送至reducer之前会对每个mapper在本地进行merge，有点类似于在MapReduce中的combiner。这样做的好处在于，在map端进行一次reduce之后，数据量会大幅度减小，从而减小传输，保证reduce端能够更快的进行结果计算。

groupByKey：groupByKey会对每一个RDD中的value值进行聚合形成一个序列(Iterator)，此操作发生在reduce端，所以势必会将所有的数据通过网络进行传输，造成不必要的浪费。同时如果数据量十分大，可能还会造成OutOfMemoryError。

通过以上对比可以发现在进行大量数据的reduce操作时候建议使用reduceByKey。不仅可以提高速度，还是可以防止使用groupByKey造成的内存溢出问题。

spark sql怎么取数据的差集



spark2.0的了解



rdd 怎么分区宽依赖和窄依赖



spark streaming 读取kafka数据的两种方式

这两种方式分别是：



kafka的数据存在内存还是磁盘



怎么解决kafka的数据丢失



答案

![https://img.mukewang.com/5b6e51600001e2e709460793.jpg](https://img.mukewang.com/5b6e51600001e2e705000420.jpg)





meta_active_imei  where ymd=20190510;

