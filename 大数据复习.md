大数据



#### hfs, yarn Hadoop 核心，运行机制， 调优





####Hive 分区分桶，开窗，UDF

Thrift, ODBC, JDBC

权限配置

```
# 临时文件目录
hdfs dfs -mkdir /tmp
hdfs dfs -chmod a+w /tmp
# 仓库目录
hdfs dfs -mkidr -p /user/hive/warehouse
hdfs dfs -chmod a+w /user/hive/warehouse
```

Hive 设置属性优先级, 由高到低

```
1、hive SET 命令
2、命令行 -hiveconf选项
3、hive-set.xml 和 hadoop 配置文件(core-site.xml, hdfs-site.xml, mapredsite.xml, yarn-site.xml)
4、Hive 默认值 和 hadoop 默认文件(core-default.xml, hdfs-default.xml, mapred-default.xml, yarn-default.xml)
```

**执行引擎**：MapReduce, apache Tez, spark

Tez 和 Spark 都是有向无环(DAG), 性能更优，将中间结果写到本地磁盘上，避免额外复制开销 

日志存放路径

```
错误日志存放路径：${java.io.tmpdir}/${user.name}/hive.log
一般上 ${java.io.tmpdir} 为/tmp
将日志目录指定到其他位置
hive -hiveconf hive.log.dir = '/tmp/${user.name}'
# 在会话中调整日志级别
hive -hiveconf hive.root.logger=DEBUG,console
```

与传统数据库的不同

```
1 读时模式 VS 写时模式
查询时对数据进行验证，读时模式。使数据加载非常迅速。
写入数据时对模式进行检查，写时模式。提高查询性能，但是加载数据会花一些时间

2 更新、事务 和 索引
hive不支持
表更新 通过把 数据变换后 放入新表实现的

hive 的索引 分为两类
compact(紧凑)：存储了每个值的HDFS块号，占用空间少
bitmap(位图)：使用压缩的位集合(bitset)来高效存储具有特殊值的行，适用于分类数据，（性别，国别）
```

![image-20190403141348719](/Users/weicheng/Library/Application Support/typora-user-images/image-20190403141348719.png)

![image-20190403141435636](/Users/weicheng/Library/Application Support/typora-user-images/image-20190403141435636.png)

hive支持原子 和 复杂数据类型

原子数据类型：数值型、布尔型、字符串、时间戳

（boolrsn, tinting, smallint, int, begint float, double, decimal, string, varchar, char, binary,

Timestamp, date）

复杂数据类型：数组、映射 和 结构

(Array, map, stuck, union)



**操作与函数**

关系操作符：

- 等值判断 ：x='a',

- 空值判断:  x IS NULL
- 模式匹配：x LIKE 'a%'

算术操作符:

逻辑操作符：



**类型转换**

- 隐式 类型转换：任何数值类型都可以隐式转换为一个范围更广的类型或温度类型，

  ​                               所有文本都可以转换为 double 或  decimal(10进制)

  ​                               Boolean 类型不能转换为其他任意数据类型

  ​                               timeStamp 和 date 可以隐式转换为文本类型

- 显示类型转换：CAST('1' as INT) 将字符串'1'转换为 整数值1， 如果转换失败，就会返回空值 NULL

#### 表

在逻辑上 由 存储的数据 和描述表中数据形式的相关元数据组成

- **托管表**：Hive 将 数据移入它的 “仓库目录”
- **外部表**：Hive 到仓库目录外的位置访问数据

```
# 加载数据到托管表
## 将 文件移动到 managed_table 表的仓库目录中 hdfs://hive仓库目录/managed_table
## 用了 LOCAL 关键字，hive会把本地文件系统的数据复制到hive仓库目录
CREATE TABLE managed_table (dummy STRING);
LOAD DATA INPATH '/usr/data.txt' INTO TABLE managed_table;

# 丢弃/[删除]一个表, 表结构 和 数据都会消失
DROP TABLE managed_table

外部表
# 加载外部表
## EXTERNAL 关键字，hive 就不会把数据移到自己的仓库目录。
CREATE EXTERNAL TABLE external_table (dummy STRING)
   LOCATION '/user/tom/external_table';
LOAD DATA INPATH '/usr/data.txt' INTO TABLE managed_table;

# 丢弃表，丢弃表时只会删除元数据（数据结构），不会碰数据
```



#### 分区 和 桶

**分区**

分区表：根据分区列对的值对表进行粗略划分的机制，具体表现在表目录下的分区文件夹。

​                 可以加快数据的查询与写入性能

分区表：静态分区表，动态分区表

分区列，是 PARTITIONED BY 字句，表中正式的列，数据文件并不包含这些列的值

```
# 在文件系统级别，分区只是表目录下嵌套的子目录
# 根据日期对日志进行分区，还可以对每个分区 进行 子分区
# 创建分区表， 使用 PARTITIONED BY 自己定义
CREATE TABLE logs (ts BIGINT, line STRING)
PARTITIONED BY (dt STRING, country STRING);

# 将数据加载到分区表的时候，要显式指定分区值
LOAD DATA LOCAL INPATH ‘input/hive/partitions/file1’
INTO TABLE logs
PARTITION (dt='2001-01-01', country='GB');

# 添加分区表， 表已经创建
ALTER TABLE table_name ADD partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ... partition_spec: : PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)

ALTER TABLE day_table ADD PARTITION (dt='2008-08-08', hour='08') location '/path/pv1.txt' PARTITION (dt='2008-08-08', hour='09') location '/path/pv2.txt';

# 删除分区
ALTER TABLE table_name DROP partition_spec, partition_spec,...

# 显示表中的分区
SHOW PARTITIONS 表名

```



**桶**

将表 或 分区 组织成桶(bucket) 有两个理由：

- 获取到更高的查询处理效率， 桶为表加上了额外的结构，在处理查询时能够利用这个结构，具体表现在 连接两个在(包含连接列的)相同列上划分了桶的表，可以使用map端连接(map-side join)高效实现
- "取样"，"采样"更高效。

```
# 使用CLUSTERED BY 子句指定划分桶所用的列 和 要划分的桶的个数
CREATE TABLE bucketed_users (id INT, name STRING)
CLUSTERED BY (id) INTO 4 BUCKETS;

# hive 对值进行哈希并将结果除以桶的个数取余数，这样任意一个桶里都会有一个随机的数据集合

# map端连接，首先两个表以相同的方式划分桶，处理左边表内某个桶的mapper 知道右边表内相匹配的行在对应的桶内，mapper 只需要获取那个桶（数据的一小部分）即可进行连接，要求两个表具有倍数关系的桶

# 桶中的数据可以根据一个或多个列进行排序，这个对每个桶的连接变成了归并排序(merge-sort)，因此可以进一步提升map 端连接的效率

# 声明一个表，使用排序桶
CREATE TABLE bucketed_user(id INT, name STRING)
CLUSTERED BY (id) SORTED BY (id, ASC) INTO 4 BUCKETS;

# 要向分桶后的表中填充成员, 需要设置 hive.enforce.bucketing
SET hive.enforce.bucketing=true

# 文件系统上，每个桶就是表(或分区)目录里面的一个文件，文件名不重要，但是 桶 n. 是按字典序排列的第n个文件。

# 一个作业产生的桶(输出文件)和reduce 任务个数相同。

# 使用TABLESAMPLE 子句 对表进行取样，这个子句会将查询限定在表的一部分桶内，而不是整个表, 返回1/4的数据行。
SELECT * FROM bucketed_users
TABLESAMPLE(BUCKET 1 OUT OF 4 ON id);

# 返回 1/2 的桶
SELECT * FROM bucketed_users
TABLESAMPLE(BUCKET 1 OUT OF 2 ON id);

```



###存储格式

- 行格式（row format）

  指行和一行中的字段如何存储，行格式的定义由SerDe定义。

  作为反序列化工具时，查询表时，将文件中的字节流形式的数据反序列化为hive内使用的对象。

  作为序列化工具时，执行INSERT或CTAS时，将hive中的数据对象序列化成字节并写到输出文件中。

- 文件格式（file format）

  指一行中字段容器的格式，纯文本文件，面向行、面向列的二进制格式。

  设置分隔文本，建表时使用 ROW FORMAT 或 STORED AS 子句

  默认分隔符 可以通过 hive.default.fileformat 属性设置

  - **默认存储格式**：分隔的文本，使用 ^A(ASCII 码中的1) Control-A

    ​                             集合类元素的默认分隔符为 字符 Control-B, 分隔 ARRAY，STRUCT, MAP 的键-值对中的元素。映射建(map key)分隔符 Control-C

    ​                             表中各行用换行符分隔​    

    

  ```
  CREATE TABLE ...
  ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\001'
    COLLECTION ITEMS TERMINATED BY '\002'
    MAP KEYS TERMINATED BY '\003'
    LINES TERMINATED BY '\n'
  STORED AS TEXTFILE;
  ```

  ​	

  - 二进制文件存储格式：顺序文件、Avro 数据文件、Parquet文件、RCFile 与 ORCFile

  ​         使用方法非常简单， 只需要通过 CREATE TABLE 语句中的 STORED AS 子句做相应声明。

  ​	面向行的格式，面向列的格式，面向行的存储格式 适合同时处理一行中很多列的情况

  ​       

  ​        hive支持两种面向行的存储格式：	

  ​    		Avro 数据文件，通用的可分割，可压缩的格式。	

  ````
  # 面向行的存储格式 AVRO， 顺序文件
  SET hive.exec.compress.output=true;
  SET avro.output.codec=snappy;
  CREATE TABLE ... STORED AS AVRO;
  CREATE TABLE ... STORED AS SEQUENCEFILE; 将顺序文件作为存储格式
  
  # 面向列的存储格式 Parquet, RCFile, ORCFile
  # 下面创建一个表 Parquet格式的复本
  CREATE TABLE users_parquet STORED AS PARQUET
  AS
  SELECT * FROM users;
  
  ````

  

导入数据

 ```hiveql
# 运行时修改hive设置, 查看属性值
SET hive.enforce.bucketing=true;
SET hive.execution.engine;
SET hive.enforce.bucketing;
# 创建表

# 将文件复制或移到表的目录中

# 将数据从一个Hive表填充到另一个

# INSERT 语句
INSETR [OVERWRITE] TABLE target
[PARTITION (dt='2001-01-01')]
SELECT col1, col2
  FROM source;
PARTITION 子句指明要数据要插入的分区
OVERWRITE 关键字表示 内容会被 SELECT语句的结果替换掉

动态分区插入, 在SELECT语句中使用分区值来动态指明分区
INSETR OVERWRITE TABLE target
PARTITION (dt)
SELECT col1, col2, dt
  FROM source;

# 多表插入, 将 FROM 子句放到最前面
FROM recoreds2
INSERT OVERWARITE TABLE stations_by_year
SELECT year, COUNT(DISTINCT station)
GROUP BY year

INSERT OVERWARITE TABLE records_by_year
SELECT year, COUNT(1)
GROUP BY year
比单条INSERT 效率高， 因为只扫描一遍表就可以生成多个不相交的输出


# 将hive 查询结果存放到一个新的表
CREATE TABLE target
AS 
SELECT col1, col2
FROM source;

 ```



表结构修改

```
# 注意 确保修改数据符合新的结构
# 用 ALTER TABLE 来重命名表， 更新表的元数据，将表目录移到新名称所对应的目录下

ALTER TABLE source_table RENAME TO target

# 添加新列, 
ALTER TABLE target ADD COLUMNS (col3 STRING);

# 修改 表结构、添加丢弃分区，修改和替换列，修改表和SerDe的属性
```

表的丢弃

```
# 删除表的数据和元数据，外部表只删除元数据
DROP TABLE table1;
# 删除表内数据，保留表定义
TRUNCATE TABLE table1;
# 创建一个 与第一个表模式相同的新表
CREATE TABLE new_table LIKE existing_table
```



查询数据

```
# 排序 和 聚集
ORDER BY： 对数据进行排序，ORDER BY 将对输入执行并行全排序
SORT BY： 为每个 reducer 产生一个排序文件
DISTRIBUTE BY： 控制某个特定的行应该到哪个reducer中
CLUSTER BY：当 SORT BY 和 DISTRIBUTE BY 使用的列相同时，可以使用缩写形式

# 调用外部脚本
TRANSFORM、 MAP、 REDUCE 子句可以在 Hive 中调用外部脚本 或 程序
---------------- 过滤低质量气象记录的 Python 脚本 is_good_quality.py -------------
#!/usr/bin/env python
import re
import sys

for line in sys.stdin:
	(year, temp, q) = line.strip().split()
	if (temp != "9999" and re.m)
		print( "%s\t%s" % (year, temp))
---------------- 使用这个脚本 --------------
ADD FILE /User/src/main/python/is_good_quality.py;
FROM records2
SELECT TRANSFORM(year, temperature, quality)
USING 'is_good_quality.py'
AS year, temperature;

---------------- reducer ----------------
FROM (
	FROM records2
	MAP year, temperature, quality
	USING 'is_good_quality.py'
	AS year, temperature) map_output
REDUCE year, temperature
USING 'max_temperature_reduce.py'
AS year, temperature;

```



连接

EXOLAIN 关键字来查看 hive 查询计划的详细信息

EXPLAIN  EXTENDED 查看更详细的计划

JOIN 子句中表的顺序很重要，大表放到最后，小表放到前面

```
SELECT * FROM sales;
Joe		2
Hank	4
Ali		0
Eve		3
Hank	2

SELECT * FROM things;
2		Tie
4		Coat
3		Hat
1		Scarf


# 内连接 JOIN ON
## 输入表之间的每次匹配都会在输出表里生成一行
SELECT sales.*, things.*
FROM sales JOIN things ON (sales.id = things.id);
Joe		2		2		Tie
Hank	4		4		Coat
Eve		3		3		Hat
Hank	2		2		Tie


# 外连接 LEFT OUTER JOIN
## 左外连接：可以让你找到连接表中不能匹配的数据行, 左侧表中无法对应右侧表的行，t右侧表对应列为NULL
SELECT sales.*, things.*
FROM sales LEFT OUTER JOIN things ON (sales.id = things.id);
Joe		2		2		Tie
Hank	4		4		Coat
Ali		0		NULL	NULL
Eve		3		3		Hat
Hank	2		2		Tie

## 右外连接 (right outer join)
SELECT sales.*, things.*
FROM sales RIGHT OUTER JOIN things ON(salea.id = things.id);
Joe		2		2		Tie
Hank	2		2		Tie
Hank	4		4		Coat
Eve		3		3		Hat
NULL	NULL	1		Scarf

## 全外连接 (full outer join), 两个表中所有输出都有对应的行;
SELECT sales.*, things.*
FROM sales FULL OUTER JOIN things ON (sales.id = things.id);
Ali		0		NULL	NULL
NULL	NULL	1		Scarf
Joe		2		2		Tie
Hank	2		2		Tie
Eve		3		3		Hat
Hank	4		4		Coat


# 半连接
## IN 子查询， 查找things 表中在 sales 表中出现过的所有商品
SELECT *
FROM things
WHERE things.id IN (SELECT id from sales);
## LEFT SEMI JOIN  实现
SELECT * FROM things LEFT SEMI JOIN sales ON (sales.id = things.id);
2	Tie
4	Coat
3	Hat

** 写 LEFT SEMI JOIN 查询时 必须遵循一个限制：右表(sales)只能在 ON 子句中出现。

# map 连接
## 有一个连接表 小的足以放入内存。hive 将较小的表放入每个 mapper 的内存来执行连接操作
SET hive.optimize.bucketmapjoin=true

```



**子查询**

内嵌在另一个 SQL 语句中的SELECT语句，hive 只允许子查询出现在SELECT 语句中的 FROM子句中，或者特殊情况下的WHERE子句中

```
SELECT station, year, AVG(max_temperature)
FROM (
SELECT station, year, MAX(temperature) as max_temperature
FROM records2
WHERE temperature != 9999 AND quality IN (0, 1, 4, 5, 9)
GROUP BY station, year
) mt
GROUP BY station, year;

# FROM 中的子查询用于计算每个气象站/日期组合中的最高气温。
# 外层 查询使用 AVG 聚集函数计算这些最高读数的均值
```



**视图**

一种 用 SELECT 语句定义的**虚表**， 以一种不同于磁盘实际存储的形式把数据呈现给用户。

现有表中的数据以一种特殊的方式进行简化和聚集以便于后期处理。

视图也可以用来限制用户，使其只能访问被授权可以看到的表的子集

Hive 中，创建视图时，并不把视图物化到存储磁盘上，视图的SELECT 语句只是在执行引用视图的语句才执行。

```
# 创建视图
## 创建视图时并不执行查询，查询只是存储在 metastore 中
## SHOW TABLES 命令的输出结果包括视图
## 用 DESCRIBE EXTENDED view_name 命令来查看某个视图的详细信息

# 创建视图1
CREATE VIEW valid_records
AS
SELECT *
FROM records2
WHERE temperature != 9999 AND quality IN (0, 1, 4, 5, 9);

# 创建视图2
CREATE VIEW max_temperatures (station, year, max_temperature)
AS
SELECT station, year, MAX(temperature) FROM valid_records
GROUP BY station, year;

# 查询, 这个查询结果和子查询的结果是一样的，
# hive 中的视图是 只读的，无法通过视图为基表 加载 或 载入数据
SELECT station, year, AVG(max_temperature)
FROM max_temperatures
GROUP BY station, year;
```



**用户自定义函数**

UDF（user-defined function）, UDAF(user-defined aggregate function ): 必须使用Java 语言编写，

- UDF (普通) ： 作用于单个数据行，且生产一个数据行作为输出，多数函数(数学、字符串)都属于此类
- UDAF (用户定义聚集函数)：接受多个输入数据行，产生一个输出数据行，count和Max 这样的函数
- UDTF (用户定义表生成函数)：作用于单个数据行，且产生多个数据行(即一个表)作为输出

#### 写 UDF

```

----------------- 剪除字符串尾字符的UDF ------------------
package com.hadoopbook.hive;

import org.apache.commons.lang.StringUtils;
improt org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

public class Strip extends UDF {
    private Text result = new Text();
    
    public Text evaluate(Text str){
        if (str == null){
            return null;
        }
        result.set(StringUtils.strip(str.toSting()));
        return result;
    }
    
    public Text evaluate(Text str, String stripChars) {
        if (str == null) {
            return null;
        }
        result.set(StringUtils.strip(str.toSting(), stripChars));
        return result;
    }
}
----------------- 将编译后的java类 打成一个JAR文件 -------
----------------- 并在 metastore中注册这个函数，并使用 CREATE FUNCTION 为它起名；
# 注册函数， 应该将jar 文件复制到HDFS上，且 USING JAR 使用 HDFS 的 URI 
CREATE FUNCTION strip AS 'com.hadoopbook.hive.Strip'
USING JAR '/path/to/hive-examples.jar'

# 像内置函数一样使用UDF，UDF对大小写不敏感
hive> SELECT strip(' bee ') FROM dummy;
bee

hive> SELECT strip('banana', 'ab') FROM dummy;
naa

# 删除 这个函数
DROP FUNCTION strip;

# 使用 TEMPORARY 关键字 可以创建一个仅在Hive会话期间有效的函数，不在metastore 中持久化存储。
ADD JAR /path/to/hive-examples.jar;
CREATE TEMPORARY FUNCTION strip AS 'com.hadoopbook.hive.Strip';


------------------UDF 需要满足的条件 --------------------
- 一个UDF 必须是org.apache.hadoop.hive.ql.exec.UDF 的子类
- 一个UDF 必须至少实现了 evaluate()方法

```



#### 写 UDAF

值是在块内进行聚集的（这些块分布在多任务中），从而实现时要能够把部分的聚集值组合成最终结果。

```
package com.hadoopbook.hive;

import org.apache.hadoop.hive.ql.exec.UDAF;
import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
import org.apache.hadoop.io.IntWritable;

public class Maximum extends UDAF {
    public static class MaximumIntUDAFEvaluator implements UDAFEvaluator {
        private IntWritable result;
        
        public void init(){
            result = null;
        }
        
        public boolean iterate (IntWritable value) {
            if (value == null) {
                return true;
            }
            if (result == null) {
                result = new IntWritable(value.get());
            } else {
                result.set(Math.max(result.get(), value.get()));
            }
            
            return true;
        }
        
        public IntWritable terminatePartial() {
            return result;
        }
        public boolean merge(IntWritable other) {
            return iterate(other);
        }
        public IntWritable terminate {
            return result;
        }
    }   
}

----------------使用 UDAF 
hive> CREATE TEMPORARY FUNCTION maximum AS 'com.hadoopbook.hive.Maximum';
hive> SELECT maximum(temperature) FROM records;
111



------------------UDAF 需要满足的条件 --------------------
必须是 org.apache.hadoop.hive.ql.exec.UDAF 的子类
包含一个或多个实现了 org.apache.hadoop.hive.ql.UDAFEvaluator的静态类

------------------一个计算函数必须实现 下面 5个 方法
- init() 方法 负责初始化计算函数并重设它的内部状态
- iterate() 方法 每次对一个新值进行聚集计算时都会调用iterate()方法。计算函数要根据聚集计算的结果更新其内部状态。
- terminatePartial() 方法 Hive 需要部分聚集结果时会调用 terminatePartial()方法。这个方法必须返回一个封装了聚集计算当前状态的对象

- merge() 方法 在Hive 决定要合并一个部分聚集值和另一个部分聚集值时会调用merge()方法，该方法接受一个对象作为输入。这个对象的类型必须 和 terminatePartial()方法的返回类型一致。

- terminate() 方法 Hive 需要最终聚集结果时会调用terminate() 方法。计算函数需要把状态作为一个值返回。

```



计算一组 double 值均值的UDAF

```
package com.hadoopbook.hive
import org.apache.hadoop.hive.ql.exec.UDAF;
import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
import org.apache.hadoop.hive.serde2.io.DoubleWritable;

public class Mean extends UDAF {
    
    public static class MeanDoubleUDAFEvaluator implements UDAFEvaluator {
        public static class PartialResult {
            double sum;
            long count;
        }
        
        private PartialResult partial;
        
        public void init() {
            partial = null;
        }
        
        public boolean iterate(DoubleWritable value){
            if (value == null){
                return true;
            }
            if (partial == null) {
                partial = new PartialResult();
            }
            partial.sum += value.get();
            partial.count++;
            return true;
        }
        
        public PartialResult terminatePartial() {
            rteurn partial;
        }
        
        public boolean merge(PartialResult other){
            if (other == null){
                return true;
            }
            if (partial == null ){
                partial = new PartialResult();
            }
            partial.sum += other.sum;
            partial.count += other.count;
            return true;
        }
        
        public DoubleWritable terminate() {
            if (partial == null) {
                return null;
            }
            return new DoubleWritable(partial.sum / partial.count);
        }
    }
}
```



**开窗**



普通的聚合函数聚合的行集是组,开窗函数聚合的行集是窗口。因此,普通的聚合函数每组(Group by)只返回一个值，而**开窗函数则可为窗口中的每行都返回一个值**。简单理解，就是**对查询的结果多出一列，这一列可以是聚合值，也可以是排序值**。
开窗函数一般分为两类,**聚合开窗函数**和**排序开窗函数**。

分析函数用于计算基于组的某种聚合值，它和聚合函数的不同之处是：**对于每个组返回多行，而聚合函数对于每个组只返回一行**。

**开窗函数**指定了分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化，

**数据窗口**



1、基础结构：

```
分析函数(如：sum(), max(), row_number()...) + 窗口子句(over函数)
```



2、over函数写法：

```
over （partition  by  cooked  order  by  createtime）
# 先根据cookieid 字段分区，相同的cookieid分为一区，每个分区内根据createtime字段排序（默认升序）
# 不加 partition by 的话把整个数据集当作一个分区，不加 order by 的话会对某些函数统计结果产生影响 如 sum()
```

![20180805102142536](/Users/weicheng/shapefiles/20180805102142536.png)



```
cookieid			createtime			pv
cookie1				2017-12-10			1
cookie1				2017-12-11			5
cookie1				2017-12-12			7
cookie1				2017-12-13			3
cookie1				2017-12-14			2
cookie1				2017-12-15			4
cookie1				2017-12-16			4
cookie2				2017-12-16			6
cookie2				2017-12-12			7
cookie3				2017-12-22			5
cookie2				2017-12-24			1
a					2017-12-01			3
b					2017-12-00			3
```

```
cookieid    createtime  pv  pv1  pv2    pv3  pv4  pv5
a           2017-12-01  3    3    3      3    3    3
b           2017-12-00  3    3    3      3    3    3
cookie1     2017-12-10  1    1    1      1    6    26
cookie1     2017-12-11  5    6    6      6    13   25
cookie1     2017-12-12  7    13  13      13   16   20
cookie1     2017-12-13  3    16  16      16   18   13
cookie1     2017-12-14  2    18  18      17   21   10
cookie1     2017-12-15  4    22  22      16   20   8
cookie1     2017-12-16  4    26  26      13   13   4
cookie2     2017-12-12  7    7    7      7    13   14
cookie2     2017-12-16  6    13  13      13   14   7
cookie2     2017-12-24  1    14  14      14   14   1
cookie3     2017-12-22  5    5    5      5     5   5
```



测试表 test1 只有三个字段 cookieid`、`createtime`、`pv

```
SELECT cookieid,createtime,pv,
SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv1, -- 默认为从起点到当前行
SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS pv2, --从起点到当前行，结果同pv1 
SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv3,   --当前行+往前3行
SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS pv4,    --当前行+往前3行+往后1行
SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS pv5   ---当前行+往后所有行  
FROM test1;
```

**如果不指定ROWS BETWEEN,默认统计窗口为从起点到当前行**;

**如果不指定ORDER BY，则将分组内所有值累加**;

```
关键是理解 ROWS BETWEEN 含义,也叫做 window子句： 
PRECEDING：往前 
FOLLOWING：往后 
CURRENT ROW：当前行 
UNBOUNDED：无边界，UNBOUNDED PRECEDING 表示从最前面的起点开始， UNBOUNDED FOLLOWING：表示到最后面的终点 
–其他AVG，MIN，MAX，和SUM用法一样
```



开窗函数相关的 URL

<https://blog.csdn.net/Abysscarry/article/details/81408265>

<https://blog.csdn.net/wangpei1949/article/details/81437574>



### HBase

> Hbase日常查询，和api操作，了解Hbase的读写流程原理、存储原理、wal机制、compact机制、 scanner 体系、hfile 格式、协处理器、布隆过滤器、预分区、rowkey 设计等。

HBase 是一个在 HDFS上开发的面向列的分布式数据库，实时随机访问超大规模数据集。

在廉价硬件构成的集群上管理超大规模的稀疏表。

应用把数据存放在带标签的表中，表由行和列组成。

表格的“单元格”由行和列的坐标交叉决定，是有版本的，

单元格的内容是未解释的字节数组，

表中行的键也是字节数组。

任何东西都可以通过表示成字符串或将二进制形式转化为长整型或直接对数据结构进行序列化，来作为键值

表中的行根据行的键值进行排序。

排序根据字节序进行。

对表的访问都要通过表的主键

##### 关系型数据库的限制

- 海量数据存储成为瓶颈，单台机器无法负载大量数据
- 单台机器IO读写请求成为海量数据存储时高并发大规模请求的瓶颈
- 数据规模扩大，业务场景开始考虑数据存储横向水平扩展。

##### 关系型数据库和非关系型数据库的典型代表

- NoSQL: hbase, redis, mongodb
- RDBMS: mysql, oracle, sql server, db2



##### HBase 数据库要点

① 它介于 NoSQL 和 RDBMS 之间，**仅能通过主键(rowkey)和主键的 range 来检索数据**

② HBase 查询数据功能很简单，**不支持 join 等复杂操作**

③ 不支持复杂的事务，**只支持行级事务**(可通过 hive 支持来实现多表 join 等复杂操作)。

④ **HBase 中支持的数据类型：byte[]（底层所有数据的存储都是字节数组）**

⑤ **主要用来存储结构化和半结构化的松散数据。**



##### 结构化、半结构化和非结构化

- **结构化**：数据结构字段含义确定，清晰，典型的如数据库中的表结构

- **半结构化**：具有一定结构，但语义不够确定，典型的如 HTML 网页，有些字段是确定的(title)， 有些不确定(table)

- **非结构化**：杂乱无章的数据，很难按照一个概念去进行抽取，无规律性

与 Hadoop 一样，HBase 目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加 计算和存储能力。



##### HBase 中的表特点

1、**大**：一个表可以有上十亿行，上百万列

2、**面向列**：面向列(族)的存储和权限控制，列(簇)独立检索。

3、**稀疏**：对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。

4、**无模式**：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一 张表中不同的行可以有截然不同的列

#### HBase表结构逻辑视图

- 内存结构
- 文件存储结构

假如我们有下面一张表

![1228818-20180328185514080-1540820263](/Users/weicheng/shapefiles/1228818-20180328185514080-1540820263.png)



**RowKey**

Rowkey的概念和mysql中的主键是完全一样的，Hbase使用Rowkey来唯一的区分某一行的数据。

由于Hbase只支持3中查询方式：

1、基于Rowkey的单行查询

2、基于Rowkey的范围扫描

3、全表扫描

Rowkey的设计很重要。设计的时候要兼顾基于Rowkey的单行查询也要键入Rowkey的范围扫描

RowKey 行键可以是任意字符串，最好是16

RowKey 保存为字节数组，HBase 会对表中的数据按照 rowkey 排序 (字典顺序)



**Column的概念**： 列，可理解成MySQL列。

**ColumnFamily的概念**

行中的列 被 分成 "列族"，就像是家族的概念，同一个列族的所有成员具有相同的前缀

[列族前缀: 列族修饰符]

一个表的列族必须作为表模式定义的一部分预先给出。新的列族成员可以随后按需加入。

面向列族的存储器，最好使所有列族成员都有相同的**访问模式** 和 大小特征。

Hbase的列族不是越多越好，官方推荐的是列族最好小于或者等于3。我们使用的场景一般是1个列族。



**TimeStamp的概念**

TimeStamp对Hbase来说至关重要，因为它是实现Hbase多版本的关键。在Hbase中使用不同的timestame来标识相同rowkey行对应的不通版本的数据。

HBase 中通过 rowkey 和 columns 确定一个存储单元为 cell。每个 cell 都保存着同一份 数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64 位整型，可以自动[显式]赋值。



每个cell 中，不同版本的数据按照时间倒序排序，最新数据排在最前面。



为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，hbase 提供了两种数据版 本回收方式：

　　**保存数据的最后 n 个版本**

　　**保存最近一段时间内的版本（设置数据的生命周期 TTL）。**

用户可以针对每个列簇进行设置。



**单元格（Cell）**

由{rowkey, column( = + ), version} 唯一确定的单元。 Cell 中的数据是没有类型的，全部是字节码形式存贮。





**区域**

区域由表中行的子集构成。每个区域由它所属于的表，它所包含的第一行及其最后一行（不包括这行）来表示。

**加锁**

对行的更新是‘原子的’

**实现结构**

结构：一个 master 节点协调管理一个或多个regionserver从属机

**master**:  HBase 主控机(master)负责启动(bootstrap)一个全新的安装，把区域分配给注册的regionserver 恢复 regionserver 的故障。master 负载很轻。

**regionserver**： 负责零个或多个区域的管理以及响应客户端的读/写请求。

​                                 负责区域的划分并通知HBase master 有了新的子区域。

**Zookeeper cluster**: HBase 依赖于 Zookeeper, Zookeeper 集合体负责 管理 如 hbase:meta 目录表的位置以及当前集群主控机地址等重要信息。

```
1、HBase 依赖于 HDFS 做底层的数据存储

2、HBase 依赖于 MapReduce 做数据计算

3、HBase 依赖于 ZooKeeper 做服务协调

4、HBase源码是java编写的，安装需要依赖JDK
```



**配置文件**：

conf/regionservers: regionserver  从属机节点列表配置文件

conf/hbase-site.xml 和 conf/hbase-env.sh :   集群站点配置



**运行中的HBase**

HBase 内部保留名为 hbase: meta  的特殊目录表。维护着当前集群上所有区域的列表、状态和位置。

hbase: meta 表中的项使用区域名作为键。

区域名 由所属的表名，区域的起始行、区域的创建时间以及对其整体进行的MD5 哈希值（表名、起始行、创建时间戳进行哈希）组成

```

```

在表名、起始行、时间戳中间用逗号分隔。MD5 哈希值则使用前后两个句号包围。

到达 Regionserver  的写操作首先追加到"提交日志"中，然后加入内存中的 memstore. 如果 memstore满，它的内容 会被“刷入” 文件系统。



##### 创建表

```
- 给表起名字
- 给表定义模式：包含表的属性和列族的列表。
              列族本身也有属性，定义模式时依次定义他们

shell 环境中 
- disable 命令可以把表设为离线
- alter 可以进行必要的修改
- enable 可以把表重新设为 '在线'

# 创建一个 名 为test的表，使其只包含一个名为data的列，表和列属性都为默认值。
create 'test', 'data'

            
```



 #### Flume 的使用



**Flume**代理构成

- source：数据来源，一般持续运行
- sink: 数据目标，HDFS， HBase, Solr
- channel： 用于连接 source 和 sink 构成的java 进程。

source 产生事件， 并将其传送给 channel, channel存储这些事件，直至转发给sink

source-channel-sink 基本 Flume组件。

Flume 由一组以分布式拓扑结构相互连接的代理构成，系统边缘的代理负责数据采集数据，并把数据转发给负责汇总的代理，然后把这些数据存储到其最终目的地。

代理通过配置来运行一组特定的 source 和 sink。 

使用Flume 所要做的主要工作就是通过配置使得各个组件融接到一起



#### 构建用于收集数据的Flume拓扑结构，作为Hadoop管道的一部分







### Sqoop 使用

主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递，可以将一个关系型数据库*（例如 ： MySQL ,Oracle ,Postgres等）*中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。

```
usage: sqoop COMMAND [ARGS]

Available commands:
  codegen            Generate code to interact with database records
  create-hive-table  Import a table definition into Hive
  eval               Evaluate a SQL statement and display the results
  export             Export an HDFS directory to a database table
  help               List available commands
  import             Import a table from a database to HDFS
  import-all-tables  Import tables from a database to HDFS
  import-mainframe   Import datasets from a mainframe server to HDFS
  job                Work with saved jobs
  list-databases     List available databases on a server
  list-tables        List available tables in a database
  merge              Merge results of incremental imports
  metastore          Run a standalone Sqoop metastore
  version            Display version information

See 'sqoop help COMMAND' for information on a specific command.

生成与数据库记录交互的代码
将表定义导入Hive
评估SQL语句并显示结果
将HDFS目录导出到数据库表
列出可用命令
将一个表从数据库导入HDFS
将全部表从数据库导入HDFS
将数据集从大型机服务器导入HDFS
使用已保存的作业
列出服务器上的可用数据库
列出数据库中的可用表
合并增量导入的结果
运行独立的Sqoop Metastore
显示版本信息
```



```
## 导入数据，将表widgets 中的数据导入到 hdfs 上 wigets目录下
sqoop import --connect jdbc:mysql://localhost/hadoopguide \
	--table widgets -m 1

# 默认 Sqoop 会将我们导入的数据保存为逗号分隔的文本文件，我们可以另外指定分隔符、字段包围字符 和转义字符。
# 可以指定 分隔符、文件格式、压缩方式
# 默认导入文件格式为文本文件，但不能保存二进制字段。
# Sqoop目前只能将 Parquet 直接加载到Hive中


## 生成代码
# Sqoop 在将源数据库的表数据写到HDFS之前，会先用生成的代码对其进行反序列化。
# 生成的类(widgets)中能够保存一条从被导入表中取出的记录，

# 使用 Sqoop 工具生成源代码
sqoop cidegen --connect jdbc:mysql://localhost/hadoopguide \

```






































